{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate QnA synthetic dataset from a Complex PDF using Azure AI Document Intelligence\n",
    "\n",
    "### Overview\n",
    "\n",
    "We process the PDF by dividing it into three parts.\n",
    "\n",
    "-   **Text-heavy** - Text-heavy PDF can be processed with open source without the need to use toolkits like Azure AI Document Intelligence or Unstructured.\n",
    "-   **Image-heavy** - Image-heavy PDF can be converted the entire page to images and let a multimodal LLM like GPT-4o summarize each page.\n",
    "-   **Mixed** - After reading the document with Azure AI Document Intelligence, we replace the image descriptions inside the figure tags with text summarized by a multimodal LLM. (Often the image descriptions are blank or have only a short caption.)\n",
    "\n",
    "![summary](../imgs/summary-creating-qna-pdf.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: python310-sdkv2\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "module_path = \"../../0_lab_preparation\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "\n",
    "from common import check_kernel\n",
    "check_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aoai_api_endpoint: https://ai-slminnvhubeastus441510290205.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-08-01-preview\n",
      "aoai_api_key: cc3183fdd2144f01850a60857789c047\n",
      "aoai_api_version: 2024-08-01-preview\n",
      "aoai_deployment_name: gpt-4o-mini\n",
      "doc_intelligence_endpoint: https://slm-innv-eastus-doc.cognitiveservices.azure.com/\n",
      "doc_intelligence_key: 5IsSs0tvkxzQY8Ff9lB1xYorT1IFWlNNQfkgt4KuixAJaqXNPHKEJQQJ99AJACYeBjFXJ3w3AAALACOGPMrg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "aoai_api_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "aoai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "aoai_deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "doc_intelligence_endpoint = os.getenv(\"AZURE_DOC_INTELLIGENCE_ENDPOINT\")\n",
    "doc_intelligence_key = os.getenv(\"AZURE_DOC_INTELLIGENCE_KEY\")\n",
    "\n",
    "if not aoai_api_version:\n",
    "    aoai_api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "if not aoai_deployment_name:\n",
    "    aoai_deployment_name = os.getenv(\"DEPLOYMENT_NAME\")\n",
    "    \n",
    "print(f\"aoai_api_endpoint: {aoai_api_endpoint}\")\n",
    "print(f\"aoai_api_key: {aoai_api_key}\")\n",
    "print(f\"aoai_api_version: {aoai_api_version}\")\n",
    "print(f\"aoai_deployment_name: {aoai_deployment_name}\")\n",
    "print(f\"doc_intelligence_endpoint: {doc_intelligence_endpoint}\")\n",
    "print(f\"doc_intelligence_key: {doc_intelligence_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read & Preprocess PDF file\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the PDFs into individual pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: Distributed training on Cloud, Language: English, Language Code: en\n"
     ]
    }
   ],
   "source": [
    "import shutil, random\n",
    "import openai\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from util.common_utils import get_language_code\n",
    "\n",
    "raw_data_dir = \"../raw_data\"\n",
    "splitted_raw_data_dir = \"splitted_raw_data\"\n",
    "file_path = f\"{raw_data_dir}/pdf/en-imagenet-training-wrote-by-daekeun.pdf\"\n",
    "\n",
    "DOMAIN = \"Distributed training on Cloud\"\n",
    "LANGUAGE = \"English\" # You can change your language here. e.g., \"Korean\", \"Japanese\", \"Chinese\"\n",
    "LANGUAGE_CODE = get_language_code(LANGUAGE)\n",
    "print(f\"Domain: {DOMAIN}, Language: {LANGUAGE}, Language Code: {LANGUAGE_CODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Only use a poration of the PDF documents for testing. If there are a lot of pages or partial processing is required, cut and save only some pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "# Open the first PDF document\n",
    "doc1 = fitz.open(file_path)\n",
    "split_pages = [(5, 25)]\n",
    "\n",
    "for idx, s in enumerate(split_pages):\n",
    "    # Create a new empty PDF document\n",
    "    doc2 = fitz.open()\n",
    "\n",
    "    # Insert the first 2 pages of doc1 into doc2\n",
    "    doc2.insert_pdf(doc1, from_page=s[0], to_page=s[1])\n",
    "\n",
    "    # Save the modified document\n",
    "    doc2.save(f\"{raw_data_dir}/part{idx}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distinguish between pages composed mainly of text, pages composed primarily of images, and pages composed of mixed text/images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder 'splitted_raw_data' and its contents have been deleted.\n",
      "### PDF Content Analysis Result:\n",
      "Text pages: [0, 1, 3, 5, 8]\n",
      "Mixed pages: [2, 4, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "from util.common_utils import delete_folder_and_make_folder\n",
    "from util.preprocess import analyze_pdf_page_content, split_pdf\n",
    "\n",
    "#file_path = f\"{raw_data_dir}/part0.pdf\"\n",
    "analyzed_pdf_result = analyze_pdf_page_content(file_path)\n",
    "delete_folder_and_make_folder(splitted_raw_data_dir)    \n",
    "\n",
    "print(\"### PDF Content Analysis Result:\")\n",
    "for content_type, pages in analyzed_pdf_result.items():\n",
    "    print(f\"{content_type} pages: {pages}\")\n",
    "    split_pdf(file_path, f\"{splitted_raw_data_dir}/{content_type}.pdf\", pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import ContentFormat\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "try:\n",
    "    client = AzureOpenAI(\n",
    "        api_key=aoai_api_key,  \n",
    "        api_version=aoai_api_version,\n",
    "        base_url=f\"{aoai_api_endpoint}/openai/deployments/{aoai_deployment_name}\",\n",
    "        max_retries=1\n",
    "    )\n",
    "except (ValueError, TypeError) as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    document_intelligence_client = DocumentIntelligenceClient(\n",
    "        endpoint=doc_intelligence_endpoint, \n",
    "        credential=AzureKeyCredential(doc_intelligence_key),\n",
    "        headers={\"x-ms-useragent\":\"sample-code-figure-understanding/1.0.0\"},\n",
    "    )\n",
    "except (ValueError, TypeError) as e:\n",
    "    print(e)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Mixed page (Images and text mixed appropriately)\n",
    "\n",
    "After reading the document with Azure AI Document Intelligence, we replace the image descriptions inside the figure tags with text summarized by a multimodal LLM. (Often the image descriptions are blank or have only a short caption.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze Document\n",
    "\n",
    "Analyze the document with Azure AI Document Intelligence to extract the text and image information.\n",
    "\n",
    "-   `crop_image_from_file()`: Crop the image from the PDF file based on the bounding box.\n",
    "-   `is_bounding_box_larger_than()`: Check if the bounding box is larger than the threshold.\n",
    "-   `image_complexity()`: Check if the image is complex or simple based on image statistics.\n",
    "-   `understand_image_with_gpt():` Summarize the image with OpenAI GPT.\n",
    "\n",
    "![post-process](../imgs/post-process1.png)\n",
    "![post-process](../imgs/post-process2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder 'pdf_mixed_tmp' and its contents have been deleted.\n",
      "Figures:\n",
      "Figure #0 has the following spans: [{'offset': 7309, 'length': 38}]\n",
      "Span #0: {'offset': 7309, 'length': 38}\n",
      "Original figure content in markdown: \n",
      "<figure>\n",
      "\n",
      "![](figures/0)\n",
      "\n",
      "</figure>\n",
      "\n",
      "\n",
      "\tNo caption found for this figure.\n",
      "\tFigure body bounding regions: {'pageNumber': 4, 'polygon': [1.2863, 1.3971, 7.2608, 1.4262, 7.2445, 4.4455, 1.2693, 4.4162]}\n",
      "is_bounding_box_larger_than - width: 5.958200000000001, height: 3.0484\n",
      "\tFigure body bounding box in (x0, y0, x1, y1): (1.2863, 1.3971, 7.2445, 4.4455)\n",
      "\tFigure 0 cropped and saved as pdf_mixed_tmp/en-imagenet-training-wrote-by-daekeun_cropped_image_0.png\n",
      "\tDescription of figure 0: The image shows a code snippet for setting up and downloading the ImageNet dataset by exporting a username and access key. Below are the steps mentioned in the code block:\n",
      "\n",
      "1. Export the ImageNet username:\n",
      "   ```\n",
      "   export IMAGENET_USERNAME=[YOUR_USERNAME]\n",
      "   ```\n",
      "\n",
      "2. Export the ImageNet access key:\n",
      "   ```\n",
      "   export IMAGENET_ACCESS_KEY=[YOUR_ACCESS_KEY]\n",
      "   ```\n",
      "\n",
      "3. Change directory to `imagenet/data`:\n",
      "   ```\n",
      "   cd imagenet/data\n",
      "   ```\n",
      "\n",
      "4. Move the `imagenet_2012_validation_synset_labels.txt` to `synsets.txt`:\n",
      "   ```\n",
      "   mv imagenet_2012_validation_synset_labels.txt synsets.txt\n",
      "   ```\n",
      "\n",
      "5. Run the `download_imagenet.sh` script in the background, and redirect its output to `download.log`:\n",
      "   ```\n",
      "   nohup bash download_imagenet.sh . synsets.txt >& download.log &\n",
      "   ```\n",
      "\n",
      "At the end of the image, there is a heading \"Method 2 (Alternative method if Method 1 does not work).\" which indicates an alternative method for downloading the ImageNet dataset manually.\n"
     ]
    }
   ],
   "source": [
    "if \"Mixed\" in analyzed_pdf_result:\n",
    "    pdf_mixed_path = f\"{splitted_raw_data_dir}/Mixed.pdf\"\n",
    "\n",
    "    with open(pdf_mixed_path, \"rb\") as f:\n",
    "        poller = document_intelligence_client.begin_analyze_document(\n",
    "            \"prebuilt-layout\", analyze_request=f, content_type=\"application/octet-stream\", \n",
    "            output_content_format=ContentFormat.MARKDOWN \n",
    "        )\n",
    "\n",
    "    result = poller.result()\n",
    "    md_content = result.content\n",
    "\n",
    "    #### Updates the content of the figure description (empty content or caption) with the image summary text generated by gpt-4o.\n",
    "    from util.preprocess import (\n",
    "        image_complexity, is_bounding_box_larger_than, crop_image_from_file, \n",
    "        understand_image_with_gpt, update_figure_description\n",
    "    )\n",
    "    output_folder = \"pdf_mixed_tmp\"\n",
    "    delete_folder_and_make_folder(output_folder)\n",
    "    language = LANGUAGE\n",
    "    max_tokens = 1024\n",
    "    input_file_path = file_path\n",
    "\n",
    "    if result.figures:\n",
    "        print(\"Figures:\")\n",
    "        for idx, figure in enumerate(result.figures):\n",
    "            figure_content = \"\"\n",
    "            img_description = \"\"\n",
    "            print(f\"Figure #{idx} has the following spans: {figure.spans}\")\n",
    "            \n",
    "            for i, span in enumerate(figure.spans):\n",
    "                print(f\"Span #{i}: {span}\")\n",
    "                figure_content += md_content[span.offset:span.offset + span.length]\n",
    "            print(f\"Original figure content in markdown: {figure_content}\")\n",
    "\n",
    "            # Note: figure bounding regions currently contain both the bounding region of figure caption and figure body\n",
    "            if figure.caption:\n",
    "                caption_region = figure.caption.bounding_regions\n",
    "                print(f\"\\tCaption: {figure.caption.content}\")\n",
    "                print(f\"\\tCaption bounding region: {caption_region}\")\n",
    "                for region in figure.bounding_regions:\n",
    "                    if region not in caption_region:\n",
    "                        print(f\"\\tFigure body bounding regions: {region}\")\n",
    "                        # To learn more about bounding regions, see https://aka.ms/bounding-region\n",
    "                        boundingbox = (\n",
    "                                region.polygon[0],  # x0 (left)\n",
    "                                region.polygon[1],  # y0 (top)\n",
    "                                region.polygon[4],  # x1 (right)\n",
    "                                region.polygon[5]   # y1 (bottom)\n",
    "                            )\n",
    "\n",
    "                        if is_bounding_box_larger_than(boundingbox):\n",
    "                            print(f\"\\tFigure body bounding box in (x0, y0, x1, y1): {boundingbox}\")\n",
    "                            cropped_image = crop_image_from_file(pdf_mixed_path, region.page_number - 1, boundingbox) # page_number is 1-indexed\n",
    "\n",
    "                            if image_complexity(cropped_image)[0] == \"Complex\":\n",
    "                                # Get the base name of the file\n",
    "                                base_name = os.path.basename(input_file_path)\n",
    "                                # Remove the file extension\n",
    "                                file_name_without_extension = os.path.splitext(base_name)[0]\n",
    "\n",
    "                                output_file = f\"{file_name_without_extension}_cropped_image_{idx}.png\"\n",
    "                                cropped_image_filename = os.path.join(output_folder, output_file)\n",
    "\n",
    "                                cropped_image.save(cropped_image_filename)\n",
    "                                print(f\"\\tFigure {idx} cropped and saved as {cropped_image_filename}\")\n",
    "\n",
    "                                try: \n",
    "                                    image_summarization = understand_image_with_gpt(client, aoai_deployment_name, cropped_image_filename, \"\", max_tokens=max_tokens, language=language)\n",
    "                                except openai.BadRequestError as e:\n",
    "                                    print(f\"BadRequestError: {e}\")\n",
    "                                    image_summarization = \"\"\n",
    "                                img_description += image_summarization\n",
    "\n",
    "                                print(f\"\\tDescription of figure {idx}: {img_description}\")\n",
    "                            else:\n",
    "                                print(f'simple image at idx {idx}')\n",
    "\n",
    "            else:\n",
    "                print(\"\\tNo caption found for this figure.\")\n",
    "                for region in figure.bounding_regions:\n",
    "                    print(f\"\\tFigure body bounding regions: {region}\")\n",
    "                    # To learn more about bounding regions, see https://aka.ms/bounding-region\n",
    "                    boundingbox = (\n",
    "                            region.polygon[0],  # x0 (left)\n",
    "                            region.polygon[1],  # y0 (top\n",
    "                            region.polygon[4],  # x1 (right)\n",
    "                            region.polygon[5]   # y1 (bottom)\n",
    "                        )\n",
    "\n",
    "                    if is_bounding_box_larger_than(boundingbox):                    \n",
    "                        print(f\"\\tFigure body bounding box in (x0, y0, x1, y1): {boundingbox}\")\n",
    "\n",
    "                        cropped_image = crop_image_from_file(input_file_path, region.page_number - 1, boundingbox) # page_number is 1-indexed\n",
    "\n",
    "                        if image_complexity(cropped_image)[0] == \"Complex\":\n",
    "                            # Get the base name of the file\n",
    "                            base_name = os.path.basename(input_file_path)\n",
    "                            # Remove the file extension\n",
    "                            file_name_without_extension = os.path.splitext(base_name)[0]\n",
    "\n",
    "                            output_file = f\"{file_name_without_extension}_cropped_image_{idx}.png\"\n",
    "                            cropped_image_filename = os.path.join(output_folder, output_file)\n",
    "                            # cropped_image_filename = f\"data/cropped/image_{idx}.png\"\n",
    "                            cropped_image.save(cropped_image_filename)\n",
    "                            print(f\"\\tFigure {idx} cropped and saved as {cropped_image_filename}\")\n",
    "\n",
    "                            try:\n",
    "                                image_summarization = understand_image_with_gpt(client, aoai_deployment_name, cropped_image_filename, \"\", max_tokens=max_tokens, language=language)\n",
    "                            except openai.BadRequestError as e:\n",
    "                                print(f\"BadRequestError: {e}\")\n",
    "                                image_summarization = \"\"\n",
    "                            img_description += image_summarization\n",
    "                            print(f\"\\tDescription of figure {idx}: {img_description}\")\n",
    "                        else:\n",
    "                            print(f'simple image at idx {idx}')\n",
    "\n",
    "            \n",
    "            md_content = update_figure_description(md_content, img_description, idx)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate chunks for mixed pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of splits (mixed case): 8\n"
     ]
    }
   ],
   "source": [
    "if \"Mixed\" in analyzed_pdf_result:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    import re\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\n",
    "            r'<!-- PageNumber=\"\\d+\" -->',\n",
    "            r\"\\n\\n\",\n",
    "            r\"\\n\",\n",
    "            \" \",\n",
    "            \".\",\n",
    "            \"\",\n",
    "        ],   \n",
    "        is_separator_regex = True,    \n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "\n",
    "    mixed_chunks = text_splitter.split_text(md_content)\n",
    "    print(\"Length of splits (mixed case): \" + str(len(mixed_chunks)))\n",
    "else:\n",
    "    mixed_chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<!-- PageHeader=\"24\\\\. 7. 22. 오전 9:52\" -->\\n\\n<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\\n\\n. Format the EBS volume, mount it on /data , and then change the owner to ec2-user: ec2-user . You may refer to\\n\\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using- volumes.html if you do not know how to mount it.\\n\\n. Download MXNet repository and TensorFlow models repository.\\n\\n$ cd /data $ git clone https://github.com/tensorflow/models.git $ git clone https://github.com/apache/incubator-mxnet.git # or, you can just type \\'pip install mxnet\\\\`\\n\\n· [Optional] For your convenience, use symbolic link such that:\\n\\n|| [ec2-user@ip-172-31-34-246 data]$ ls -1 || | | |||\\n| 합계 13864 ||| | | |||\\n| - | - | - | - | - | - | - | - |\\n| drwxrwxr-x 4 ec2-user ec2-user ||| 37 | 9월 | 16 02:20 im2rec |||\\n||| rwxrwxrwx 1 ec2-user ec2-user   |||| 41 9% 10 02:38 imagenet -> /data/models/research/inception/inception    ||\\n||| drwxrwxr-x 7 ec2-user ec2-user   | 249 |||| 9월 10 02:22 models    |\\n||| Irwxrwxrwx 1 ec2-user ec2-user   | 44 |||| 9 16 00:29 mxnet -> /usr/local/lib/python2. 7/site-packages/mxnet    |\\n||| -rw ------- 1 ec2-user ec2-user 14193041  | |||| 9월 17 23:46 nohup. out    |\\n||| drwxrwxr-x 4 ec2-user ec2-user   | 98 |||| 9월 16 00:52 opencv    |\\n| [ec2-user@ip-172-31-34-246 data]$ ||||||||\\n\\n. [Important Step] You need to install OpenCV also. (Both 3.x and 4.x work well). If you do not install OpenCV, then you cannot convert ImageNet raw data to RecordIO files since im2rec. py utilizes some OpenCV functions. You may refer to https://www.pyimagesearch.com/2018/08/15/how-to-install-opencv- 4-on-ubuntu/.\\n\\n. [Caution] I strongly recommend to use Python2 instead of Python3 because many codes of Tensorflow models repository does not work on Python3. Please refer to https://stackoverflow.com/questions/38546672/inception- build-imagenet-data-py-typeerror-rgb-has-type-class-str-but-ex.\\n\\n\\n# Downloading ImageNet',\n",
       " '# Downloading ImageNet\\n\\nPlease note that ImageNet server is sometimes unstable so download speed is not fast, taking 4 to 5 days.\\n\\n\\n# Method 1\\n\\n· Go to http://www.image-net.org/, sign up, and get your own username and access key.\\n\\n<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\\n\\n<!-- PageNumber=\"3/9\" -->\\n\\n<!-- PageHeader=\"24\\\\. 7. 22. 오전 9:52\" -->\\n\\n<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\\n\\n· Extract the training set\\n\\n$ mkdir train $ mv ILSVRC2012\\\\_img\\\\_train. tar train $ cd train $ tar xf ILSVRC2012\\\\_img\\\\_train. tar $ find . - name \" \\\\*. tar\" | while read NAME ; do mkdir -p \"${NAME%. tar}\"; tar -xvf \"${NAME}\" -C \"${NAME%. tar}\"; rm -f \"${NAME}\"; done\\n\\n. After extracting the training set, check if the number of directories is 1,000 (class 1 is n01728572 and class 1000 is n15075141).\\n\\n· Extract bounding boxes\\n\\n$ mkdir bounding\\\\_boxes $ mv ILSVRC2012\\\\_bbox\\\\_train\\\\_v2.tar.gz bounding\\\\_boxes $ mv ILSVRC2012\\\\_bbox\\\\_val\\\\_v3.tgz bounding\\\\_boxes $ cd bounding\\\\_boxes $ tar xzf ILSVRC2012\\\\_bbox\\\\_val\\\\_v3.tgz $ mkdir train $ mv ILSVRC2012\\\\_bbox\\\\_train\\\\_v2. tar.gz train $ cd train $ tar xzf ILSVRC2012\\\\_bbox\\\\_train\\\\_v2.tar.gz',\n",
       " '| -rw-rw-r -- ||| 1 ec2-user ec2-user   || 19537608 10월 | ||| 1 00:01 ILSVRC2012\\\\_bbox\\\\_train\\\\_v2.tar.gz   |\\n| - | - | - | - | - | - | - | - | - | - |\\n| -rw-rw-r -- ||| 1 ec2-user ec2-user   | 2221290 || 9월 | 18 || 2012 ILSVRC2012\\\\_bbox\\\\_val\\\\_v3.tgz  |\\n| -rw-rw-r -- |||| | 1 ec2-user ec2-user 147897477120    | 6월 | 14 || 2012 ILSVRC2012\\\\_img\\\\_train.tar  |\\n| -rw-rw-r -- ||| 1 ec2-user ec2-user   | | 6744924160 | 6월 | 14 || 2012 ILSVRC2012\\\\_img\\\\_val.tar  |\\n||| drwxrwxr-x 1002 ec2-user ec2-user |  | | 32768 | 9월 ||| 17 01:36 bounding\\\\_boxes   |\\n| -rw-r -- r -- || 1 root  | root | | 29709928 |||| 9% 17 06:02 imagenet\\\\_2012\\\\_bounding\\\\_boxes.csv    |\\n|| || drwxrwxr-x 1002 ec2-user ec2-user   | | 32768 |||| 9월 17 02:29 train    |\\n|| || drwxrwxr-x 1002 ec2-user ec2-user   | | 2691072 |||| 9@ 17 05:54 validation    |\\n| (base) [ec2-user@ip-172-31-35-5 data]$ |||||| | |||\\n\\n\\n# Data Transformation\\n\\n\\n# RecordIO format\\n\\n. Use im2rec. py the same way Simon did. (https://medium.com/@julsimon/imagenet-part-1-going-on-an-adventure- c0a62976dc72). It takes 1.5 days on the t2. large instance. I think he did some typos (ImageNet baseline usually uses 224x224 size image, but he uses 480×480).\\n\\n\\n# TFRecord format\\n\\n<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\\n\\n<!-- PageNumber=\"5/9\" -->\\n\\n<!-- PageHeader=\"24\\\\. 7. 22. 오전 9:52\" -->',\n",
       " '<!-- PageNumber=\"5/9\" -->\\n\\n<!-- PageHeader=\"24\\\\. 7. 22. 오전 9:52\" -->\\n\\n. Create an EC2 instance for Training (Deep Learning AMI (Ubuntu 16.04) or Deep Learning AMI (Amazon Linux)). p3.16xlarge or p3dn.24xlarge is recommended if you need to do distributed GPU training using Uber\\'s Horovod or Tensorflow\\'s DistributedStrategy). Please also note that the default root volume size is 75GB, but I recommend you to increase 100GB since training logs and model checkpoints are stored in the root volume if you do not modify training configuration. If you not want to increase the volume size, then you can delete some conda environments such as Theano, Chainer, Caffe, and Caffe2 after logging in to the EC2 instance.\\n\\no https://aws.amazon.com/ko/getting-started/tutorials/get-started-dlami/\\n\\n. If you want to train on distributed GPUs, then you need to create multiple GPU instances with the same setting. For example, the below figure shows 8 p3dn.24xlarge instances.',\n",
       " '. If you want to train on distributed GPUs, then you need to create multiple GPU instances with the same setting. For example, the below figure shows 8 p3dn.24xlarge instances.\\n\\n| a | search : POC\\\\_HU24 | Add filter | | | | |\\n| :unselected: | Name | | Instance ID - 4 | Instance Type | Availability Zone | Instance State v |\\n| - | - | - | - | - | - | - |\\n| :unselected: | POC\\\\_HU24\\\\_81 | | i-089bb90e91fef7b09 | p3dn.24xlarge | us-west-2c | :selected: running |\\n| :unselected: | POC\\\\_HU24\\\\_82 | | i-09be131f79506dcc1 | p3dn.24xlarge | us-west-2c | :unselected: running |\\n| :unselected: | POC\\\\_HU24\\\\_83 | | i-0c44553f8570af264 | p3dn.24xlarge | us-west-2c | :unselected: running |\\n| :unselected: | POC\\\\_HU24\\\\_84 | | i-0d8f1a29d7864e892 | p3dn.24xlarge | us-west-2c | :unselected: running |\\n| :unselected: | POC\\\\_HU24\\\\_85 | | i-0de84adf899462171 | p3dn.24xlarge | us-west-2c | :unselected: running |\\n| :unselected: | POC\\\\_HU24\\\\_86 | | i-0e56678cc29ad0de8 | p3dn.24xlarge | us-west-2c | :unselected: running |\\n| :unselected: | POC\\\\_HU24\\\\_87 | | i-Of4912fb1d7760a1b | p3dn.24xlarge | us-west-2c | :unselected: running |\\n| :unselected: | POC\\\\_HU24\\\\_88 | | i-Of4ddd1c5bbfd4dd7 | p3dn.24xlarge | us-west-2c | :selected: running |\\n\\n· Please refer to the website for the remaining steps; https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-horovod- tensorflow.html. Note that all code and all feature sets(TFRecord and RecordIO) must be on the same path on each server.\\n\\n<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\\n\\n<!-- PageNumber=\"7/9\" -->\\n\\n<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\\n\\n<!-- PageHeader=\"24\\\\. 7. 22. 오전 9:52\" -->\\n\\n\\n# · After training, please check the training log and evaluation log by checking imagenet\\\\_resnet folder:\\n\\n<figure>',\n",
       " '<!-- PageHeader=\"24\\\\. 7. 22. 오전 9:52\" -->\\n\\n\\n# · After training, please check the training log and evaluation log by checking imagenet\\\\_resnet folder:\\n\\n<figure>\\n\\n![](figures/0)<!-- FigureContent=\"The image shows a code snippet for setting up and downloading the ImageNet dataset by exporting a username and access key. Below are the steps mentioned in the code block:\\n\\n1. Export the ImageNet username:\\n   ```\\n   export IMAGENET_USERNAME=[YOUR_USERNAME]\\n   ```\\n\\n2. Export the ImageNet access key:\\n   ```\\n   export IMAGENET_ACCESS_KEY=[YOUR_ACCESS_KEY]\\n   ```\\n\\n3. Change directory to `imagenet/data`:\\n   ```\\n   cd imagenet/data\\n   ```\\n\\n4. Move the `imagenet_2012_validation_synset_labels.txt` to `synsets.txt`:\\n   ```\\n   mv imagenet_2012_validation_synset_labels.txt synsets.txt\\n   ```\\n\\n5. Run the `download_imagenet.sh` script in the background, and redirect its output to `download.log`:\\n   ```\\n   nohup bash download_imagenet.sh . synsets.txt >& download.log &\\n   ```\\n\\nAt the end of the image, there is a heading \"Method 2 (Alternative method if Method 1 does not work).\" which indicates an alternative method for downloading the ImageNet dataset manually.\" --></figure>\\n\\n\\n<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->',\n",
       " '<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\\n\\n| :unselected: | | | || ubuntu@ip-172-31-3-51: \\\\~ /examples/horovod/tensorflow/imagenet\\\\_resnet (ssh)  |\\n| - | - | - | - | - | - |\\n| ubuntu@ip-172-31-3-51 :\\\\~ /examples/horovod/tensorflow/imagenet\\\\_resnet$ ls -l ||||||\\n| total 646280 | | ||||\\n| 6-rw-rw-r -- 1 ubuntu ubuntu ||||| 89 Sep 23 02:54 checkpoint     |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 18880 Oct 1 01:22 eval\\\\_hvd\\\\_train. log     |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 21227745 Sep 23 03:01 events.out. tfevents. 1569199858. ip-172-31-3-51     |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 9287777 Sep 23 00:51 graph.pbtxt     |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 18880 Sep 23 03:03 hvd\\\\_train. log     |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 8 Sep 23 00:51 model.ckpt-0.data-00000-of-00002     |\\n|||||| -rw-rw-r -- 1 ubuntu ubuntu 204668736 Sep 23 00:51 model.ckpt-0.data-00001-of-00002      |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 17114 Sep 23 00:51 model. ckpt-0. index     |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 5709416 Sep 23 00:51 model. ckpt-0.meta     |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 8 Sep 23 01:53 model.ckpt-10000.data-00000-of-00002     |\\n|||||| -rw-rw-r -- 1 ubuntu ubuntu 204668736 Sep 23 01:53 model.ckpt-10000.data-00001-of-00002      |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 17114 Sep 23 01:53 model.ckpt-10000. index     |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 5709416 Sep 23 01:53 model.ckpt-10000.meta     |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 8 Sep 23 02:54 model.ckpt-20000.data-00000-of-00002     |\\n|||||| -rw-rw-r -- 1 ubuntu ubuntu 204668736 Sep 23 02:54 model.ckpt-20000.data-00001-of-00002      |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 17114 Sep 23 02:54 model.ckpt-20000. index     |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 5709416 Sep 23 02:54 model.ckpt-20000.meta     |\\n| (ubuntu@ip-172-31-3-51 :\\\\~ /examples/horovod/tensorflow/imagenet\\\\_resnet$ ||||||\\n\\n\\n## · vd\\\\_train\\\\_log (32 GPUS; 4 p3dn.24xlarge instances)',\n",
       " '## · vd\\\\_train\\\\_log (32 GPUS; 4 p3dn.24xlarge instances)\\n\\n\\\\- Step Epoch Speed Loss FinLoss LR - 0 0.0 952.2 6.923 8.262\\n\\n0.00100 - 1 0.0 2686.6 6.928 8.267 0.00305 - 50 0.3 22243.7\\n\\n6.586 7.919 0.10353 14000 89.5 21021.1 0.750 1.152 -\\n\\n0.00012 - 14050 89.8 21818.7 0.583 0.985 0.00002 - Finished in 5289. 161954164505\\n\\n\\n## · eval\\\\_hvd\\\\_train.log (32 GPUS; 4 p3dn.24xlarge instances)\\n\\nubuntu@ip-172-31-3-51 :\\\\~ /examples/horovod/tensorflow$ cat eval\\\\_hvd\\\\_train\\\\_gpu32.log\\n\\nPY3.6.5 |Anaconda, Inc. | (default, Apr 29 2018, 16:14:56)\\n\\n[GCC 7.2.0]TF1.13.1\\n\\nHorovod size: 8\\n\\nUsing data from: /home/ubuntu/data1/tf-imagenet/ Evaluating Validation dataset size: 50000\\n\\nstep epoch top1 top5 loss checkpoint\\\\_time(UTC) 14075 90.2 75.821 92.90\\n\\n0.92 2019-09-20 07:50:57\\n\\nFinished evaluation\\n\\n<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\\n\\n<!-- PageNumber=\"8/9\" -->']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Text-heavy\n",
    "\n",
    "Text-heavy PDFs can be processed with open source without the need to use toolkits like Azure AI Document Intelligence or Unstructured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0\n",
      "page_content='[Hands-on] Fast Training\n",
      "ImageNet on on-demand EC2\n",
      "GPU instances with Horovod\n",
      "💻\n",
      "Author: Daekeun Kim (daekeun@amazon.com)\n",
      "Goal\n",
      "This document is for people who need distributed GPU training using Horovod for\n",
      "experimental purposes. Many steps are similar to what mentioned in Julien\n",
      "Simon’s article(\n",
      ") and AWS\n",
      "Documentation(\n",
      "). So I recommend you to view these articles first. If there\n",
      "are some things that aren’t going well (e.g., Downloading the dataset does not\n",
      "work, How to convert the raw data to the TFRecord feature set?, How to fix the\n",
      "error ModuleNotFoundError: No module named 'cv2'? ) please refer this\n",
      "document.\n",
      "https://medium.com/@julsimon/imagenet-part-1-going-on-an-\n",
      "adventure-c0a62976dc72\n",
      "https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-\n",
      "horovod-tensorflow.html\n",
      "Introduction\n",
      "For data preparation and data transformation, we do not need to use a GPU\n",
      "instance such as p2 and p3. Instead, we can start much cheaper instances like \n",
      "t2.large  instance with 1.0TB EBS volume.\n",
      "For distributed training, we need to use multiple GPU instances like p2, p3, g3 and\n",
      "g4.\n",
      "You can skip step 1 if you do not want to invent the wheel again because I have\n",
      "stored everything in my s3 bucket.' metadata={'source': 'splitted_raw_data/Text.pdf', 'file_path': 'splitted_raw_data/Text.pdf', 'page': 0, 'total_pages': 5, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}\n",
      "================================================================================\n",
      "Chunk 1\n",
      "page_content='g4.\n",
      "You can skip step 1 if you do not want to invent the wheel again because I have\n",
      "stored everything in my s3 bucket.\n",
      "24. 7. 22. 오전 9:52\n",
      "[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\n",
      "https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\n",
      "1/9' metadata={'source': 'splitted_raw_data/Text.pdf', 'file_path': 'splitted_raw_data/Text.pdf', 'page': 0, 'total_pages': 5, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}\n",
      "================================================================================\n",
      "Chunk 2\n",
      "page_content='•\n",
      "s3://dataset-image/imagenet/raw  (raw jpeg)\n",
      "•\n",
      "s3://dataset-image/imagenet/tfrecord  (TFRecord before resizing)\n",
      "•\n",
      "s3://dataset-image/imagenet/tfrecord-resized  (TFRecord after resizing to\n",
      "224x224)\n",
      "•\n",
      "s3://dataset-image/imagenet/recordio  (RecordIO after resizing to\n",
      "256x256)\n",
      "◦The reason I did not resize 224x224 is that the below article shows\n",
      "different validation accuracy for resizing strategy.\n",
      "◦https://forums.fast.ai/t/impact-of-image-resizing-on-model-training-time-\n",
      "and-performance/1980\n",
      "Please let me know if you want to access the bucket because I did not grant any\n",
      "public access.\n",
      "Step 1. Downloading and Transformation\n",
      "Setting up an EC2 instance for Data Transformation\n",
      "• Create an EC2 instance for storing ImageNet dataset (Ubuntu 18.04 or 16.04.\n",
      "Linux is also available). t2.micro  is also available, but t2.large  is\n",
      "recommended due to memory size. Note that we do not need large storage\n",
      "size since we will make another EBS volume to attach the EC2 instance.\n",
      "• Create an EBS volume (1.0TB) for ImageNet dataset and then attach the\n",
      "volume it to your EC2 instance. ImageNet consists of 138GB for training set\n",
      "and 6.3GB for validation set, but we need an additional space since we need to' metadata={'source': 'splitted_raw_data/Text.pdf', 'file_path': 'splitted_raw_data/Text.pdf', 'page': 1, 'total_pages': 5, 'format': 'PDF 1.7', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': '', 'creationDate': '', 'modDate': '', 'trapped': ''}\n",
      "================================================================================\n",
      "Length of splits (text-heay case): 9\n"
     ]
    }
   ],
   "source": [
    "if \"Text\" in analyzed_pdf_result:\n",
    "    from langchain_community.document_loaders.pdf import PyMuPDFLoader\n",
    "    from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "    pdf_text_path = f\"{splitted_raw_data_dir}/Text.pdf\"\n",
    "    loader = PyMuPDFLoader(pdf_text_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1200, \n",
    "        chunk_overlap=200\n",
    "    )\n",
    "\n",
    "    text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    for idx, chunk in enumerate(text_chunks):\n",
    "        print(f\"Chunk {idx}\\n{chunk}\")\n",
    "        print(\"=\"*80)\n",
    "        if idx == 2:\n",
    "            break\n",
    "\n",
    "    text_chunks = [d.page_content for d in text_chunks]\n",
    "    print(\"Length of splits (text-heay case): \" + str(len(text_chunks)))\n",
    "else:\n",
    "    text_chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3: Image-heavy\n",
    "\n",
    "Image-heavy PDF can be converted the entire page to images and let a multimodal LLM like GPT-4o summarize each page.\n",
    "\n",
    "### Preprocess Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if \"Image\" in analyzed_pdf_result:\n",
    "    import fitz\n",
    "    from glob import glob\n",
    "\n",
    "    image_dir = \"./pdf_image_tmp\"\n",
    "    delete_folder_and_make_folder(image_dir) \n",
    "\n",
    "    pdf_image_path = f\"{splitted_raw_data_dir}/Image.pdf\"\n",
    "    doc = fitz.open(pdf_image_path)\n",
    "    #clip_x, clip_y = 10, 45\n",
    "    clip_x, clip_y = 10, 10\n",
    "\n",
    "    for i, page in enumerate(doc):\n",
    "        x, y, w, h = page.rect\n",
    "        clip = fitz.Rect(x+clip_x, y+clip_y, w-clip_x, h-clip_y)\n",
    "        page.set_cropbox(clip)\n",
    "        pix = page.get_pixmap()\n",
    "        pix.save(f\"{image_dir}/page_{i:03d}.jpg\")\n",
    "\n",
    "    images = sorted(glob(os.path.join(image_dir, \"*.jpg\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no image heavy documents\n"
     ]
    }
   ],
   "source": [
    "if \"Images\" in analyzed_pdf_result:\n",
    "    print(f\"extracted {len(images)} images\")\n",
    "else:\n",
    "    print(f\"no image heavy documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "max_tokens = 1024\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=max_tokens,\n",
    "    openai_api_version=aoai_api_version,\n",
    "    azure_deployment=aoai_deployment_name\n",
    ")\n",
    "\n",
    "human_prompt_main = f\"Given image, give a concise summary in {LANGUAGE}. Don't insert any XML tag such as <text> and </text> when answering.\"\n",
    "\n",
    "system_prompt = \"You are an assistant tasked with describing table or image, specialized in Smartphone product.\"\n",
    "system_message_template = SystemMessagePromptTemplate.from_template(system_prompt)\n",
    "human_prompt = [\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": \"data:image/png;base64,\" + \"{image_base64}\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": human_prompt_main\n",
    "    },\n",
    "]\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message_template,\n",
    "        human_message_template\n",
    "    ]\n",
    ")\n",
    "\n",
    "summarize_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No image heavy doc in analyzed pdfs\n",
      "CPU times: user 102 μs, sys: 17 μs, total: 119 μs\n",
      "Wall time: 124 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if \"Image\" in analyzed_pdf_result:\n",
    "    from util.preprocess import encode_image_base64\n",
    "    #images = glob(os.path.join(image_path, \"*.jpg\"))\n",
    "    base64_images = [encode_image_base64(img_path) for img_path in images]\n",
    "    image_summaries = summarize_chain.batch(base64_images, {\"max_concurrency\": 8})\n",
    "    image_summaries = remove_short_sentences(image_summaries)\n",
    "    print(\"Length of image_summaries (image-heavy case): \" + str(len(image_summaries)))\n",
    "else:\n",
    "    print(\"No image heavy doc in analyzed pdfs\")\n",
    "    image_summaries = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see all chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      All chunks from analyzed PDF documents:\n",
      "      Mixed chunks: 8 chunks. \n",
      "      Text heavy chunks: 9 chunks.\n",
      "      Images extracted: 0\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "      All chunks from analyzed PDF documents:\n",
    "      Mixed chunks: {len(mixed_chunks)} chunks. \n",
    "      Text heavy chunks: {len(text_chunks)} chunks.\n",
    "      Images extracted: {len(image_summaries)}\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construct QnA Pairs\n",
    "\n",
    "---\n",
    "\n",
    "### Option 1.\n",
    "\n",
    "Leverage the `azure-ai-generative` package. The `QADataGenerator` class in this package makes it easy to generate QnA synthetic questions. However, using this class as is has the disadvantage of not being able to use custom prompts, so we inherited from it and created the `CustomQADataGenerator` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from util.qa import CustomQADataGenerator\n",
    "model_config = {\n",
    "    \"deployment\": aoai_deployment_name,\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"max_tokens\": 2000,\n",
    "}\n",
    "\n",
    "qa_generator = CustomQADataGenerator(model_config=model_config, templates_dir=f\"./prompt_template/{LANGUAGE_CODE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "import os\n",
    "from azure.ai.generative.synthetic.qa import QAType\n",
    "concurrency = 6  # number of concurrent calls\n",
    "sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "#qa_type = QAType.CONVERSATION\n",
    "qa_type = QAType.LONG_ANSWER\n",
    "\n",
    "async def generate_async(text: str) -> Dict:\n",
    "    async with sem:\n",
    "        return await qa_generator.generate_async(\n",
    "            text=text,\n",
    "            qa_type=qa_type,\n",
    "            num_questions=3,  # Number of questions to generate per text\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_batch = mixed_chunks + text_chunks + image_summaries\n",
    "results = await asyncio.gather(*[generate_async(text) for text in input_batch], return_exceptions=True)\n",
    "\n",
    "question_answer_list = []\n",
    "for result in results:\n",
    "    if isinstance(result, Exception):\n",
    "        raise result  # exception raised inside generate_async()\n",
    "    question_answer_list.append(result[\"question_answers\"])\n",
    "\n",
    "print(\"Successfully generated QAs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2.\n",
    "\n",
    "You write the entire sequence of code to create a QnA dataset without using a separate toolkit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from util.qa_pair import get_qna_prompt_template, QAPair\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=1024,\n",
    "    openai_api_version=aoai_api_version,\n",
    "    azure_deployment=aoai_deployment_name\n",
    ")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=QAPair)\n",
    "prompt = get_qna_prompt_template(LANGUAGE)\n",
    "\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_batch = []\n",
    "\n",
    "for doc in mixed_chunks:\n",
    "    dic = {\"context\": doc, \"domain\": DOMAIN, \"num_questions\": \"3\"}\n",
    "    input_batch.append(dic)\n",
    "\n",
    "for doc in text_chunks:\n",
    "    dic = {\"context\": doc, \"domain\": DOMAIN, \"num_questions\": \"3\"}\n",
    "    input_batch.append(dic)\n",
    "\n",
    "for doc in image_summaries:\n",
    "    dic = {\"context\": doc, \"domain\": DOMAIN, \"num_questions\": \"3\"}\n",
    "    input_batch.append(dic)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'context': '<!-- PageHeader=\"24\\\\. 7. 22. 오전 9:52\" -->\\n\\n<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\\n\\n. Format the EBS volume, mount it on /data , and then change the owner to ec2-user: ec2-user . You may refer to\\n\\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using- volumes.html if you do not know how to mount it.\\n\\n. Download MXNet repository and TensorFlow models repository.\\n\\n$ cd /data $ git clone https://github.com/tensorflow/models.git $ git clone https://github.com/apache/incubator-mxnet.git # or, you can just type \\'pip install mxnet\\\\`\\n\\n· [Optional] For your convenience, use symbolic link such that:\\n\\n|| [ec2-user@ip-172-31-34-246 data]$ ls -1 || | | |||\\n| 합계 13864 ||| | | |||\\n| - | - | - | - | - | - | - | - |\\n| drwxrwxr-x 4 ec2-user ec2-user ||| 37 | 9월 | 16 02:20 im2rec |||\\n||| rwxrwxrwx 1 ec2-user ec2-user   |||| 41 9% 10 02:38 imagenet -> /data/models/research/inception/inception    ||\\n||| drwxrwxr-x 7 ec2-user ec2-user   | 249 |||| 9월 10 02:22 models    |\\n||| Irwxrwxrwx 1 ec2-user ec2-user   | 44 |||| 9 16 00:29 mxnet -> /usr/local/lib/python2. 7/site-packages/mxnet    |\\n||| -rw ------- 1 ec2-user ec2-user 14193041  | |||| 9월 17 23:46 nohup. out    |\\n||| drwxrwxr-x 4 ec2-user ec2-user   | 98 |||| 9월 16 00:52 opencv    |\\n| [ec2-user@ip-172-31-34-246 data]$ ||||||||\\n\\n. [Important Step] You need to install OpenCV also. (Both 3.x and 4.x work well). If you do not install OpenCV, then you cannot convert ImageNet raw data to RecordIO files since im2rec. py utilizes some OpenCV functions. You may refer to https://www.pyimagesearch.com/2018/08/15/how-to-install-opencv- 4-on-ubuntu/.\\n\\n. [Caution] I strongly recommend to use Python2 instead of Python3 because many codes of Tensorflow models repository does not work on Python3. Please refer to https://stackoverflow.com/questions/38546672/inception- build-imagenet-data-py-typeerror-rgb-has-type-class-str-but-ex.\\n\\n\\n# Downloading ImageNet',\n",
       "  'domain': 'Distributed training on Cloud',\n",
       "  'num_questions': '3'},\n",
       " {'context': '# Downloading ImageNet\\n\\nPlease note that ImageNet server is sometimes unstable so download speed is not fast, taking 4 to 5 days.\\n\\n\\n# Method 1\\n\\n· Go to http://www.image-net.org/, sign up, and get your own username and access key.\\n\\n<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\\n\\n<!-- PageNumber=\"3/9\" -->\\n\\n<!-- PageHeader=\"24\\\\. 7. 22. 오전 9:52\" -->\\n\\n<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\\n\\n· Extract the training set\\n\\n$ mkdir train $ mv ILSVRC2012\\\\_img\\\\_train. tar train $ cd train $ tar xf ILSVRC2012\\\\_img\\\\_train. tar $ find . - name \" \\\\*. tar\" | while read NAME ; do mkdir -p \"${NAME%. tar}\"; tar -xvf \"${NAME}\" -C \"${NAME%. tar}\"; rm -f \"${NAME}\"; done\\n\\n. After extracting the training set, check if the number of directories is 1,000 (class 1 is n01728572 and class 1000 is n15075141).\\n\\n· Extract bounding boxes\\n\\n$ mkdir bounding\\\\_boxes $ mv ILSVRC2012\\\\_bbox\\\\_train\\\\_v2.tar.gz bounding\\\\_boxes $ mv ILSVRC2012\\\\_bbox\\\\_val\\\\_v3.tgz bounding\\\\_boxes $ cd bounding\\\\_boxes $ tar xzf ILSVRC2012\\\\_bbox\\\\_val\\\\_v3.tgz $ mkdir train $ mv ILSVRC2012\\\\_bbox\\\\_train\\\\_v2. tar.gz train $ cd train $ tar xzf ILSVRC2012\\\\_bbox\\\\_train\\\\_v2.tar.gz',\n",
       "  'domain': 'Distributed training on Cloud',\n",
       "  'num_questions': '3'},\n",
       " {'context': '| -rw-rw-r -- ||| 1 ec2-user ec2-user   || 19537608 10월 | ||| 1 00:01 ILSVRC2012\\\\_bbox\\\\_train\\\\_v2.tar.gz   |\\n| - | - | - | - | - | - | - | - | - | - |\\n| -rw-rw-r -- ||| 1 ec2-user ec2-user   | 2221290 || 9월 | 18 || 2012 ILSVRC2012\\\\_bbox\\\\_val\\\\_v3.tgz  |\\n| -rw-rw-r -- |||| | 1 ec2-user ec2-user 147897477120    | 6월 | 14 || 2012 ILSVRC2012\\\\_img\\\\_train.tar  |\\n| -rw-rw-r -- ||| 1 ec2-user ec2-user   | | 6744924160 | 6월 | 14 || 2012 ILSVRC2012\\\\_img\\\\_val.tar  |\\n||| drwxrwxr-x 1002 ec2-user ec2-user |  | | 32768 | 9월 ||| 17 01:36 bounding\\\\_boxes   |\\n| -rw-r -- r -- || 1 root  | root | | 29709928 |||| 9% 17 06:02 imagenet\\\\_2012\\\\_bounding\\\\_boxes.csv    |\\n|| || drwxrwxr-x 1002 ec2-user ec2-user   | | 32768 |||| 9월 17 02:29 train    |\\n|| || drwxrwxr-x 1002 ec2-user ec2-user   | | 2691072 |||| 9@ 17 05:54 validation    |\\n| (base) [ec2-user@ip-172-31-35-5 data]$ |||||| | |||\\n\\n\\n# Data Transformation\\n\\n\\n# RecordIO format\\n\\n. Use im2rec. py the same way Simon did. (https://medium.com/@julsimon/imagenet-part-1-going-on-an-adventure- c0a62976dc72). It takes 1.5 days on the t2. large instance. I think he did some typos (ImageNet baseline usually uses 224x224 size image, but he uses 480×480).\\n\\n\\n# TFRecord format\\n\\n<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\\n\\n<!-- PageNumber=\"5/9\" -->\\n\\n<!-- PageHeader=\"24\\\\. 7. 22. 오전 9:52\" -->',\n",
       "  'domain': 'Distributed training on Cloud',\n",
       "  'num_questions': '3'},\n",
       " {'context': '<!-- PageNumber=\"5/9\" -->\\n\\n<!-- PageHeader=\"24\\\\. 7. 22. 오전 9:52\" -->\\n\\n. Create an EC2 instance for Training (Deep Learning AMI (Ubuntu 16.04) or Deep Learning AMI (Amazon Linux)). p3.16xlarge or p3dn.24xlarge is recommended if you need to do distributed GPU training using Uber\\'s Horovod or Tensorflow\\'s DistributedStrategy). Please also note that the default root volume size is 75GB, but I recommend you to increase 100GB since training logs and model checkpoints are stored in the root volume if you do not modify training configuration. If you not want to increase the volume size, then you can delete some conda environments such as Theano, Chainer, Caffe, and Caffe2 after logging in to the EC2 instance.\\n\\no https://aws.amazon.com/ko/getting-started/tutorials/get-started-dlami/\\n\\n. If you want to train on distributed GPUs, then you need to create multiple GPU instances with the same setting. For example, the below figure shows 8 p3dn.24xlarge instances.',\n",
       "  'domain': 'Distributed training on Cloud',\n",
       "  'num_questions': '3'},\n",
       " {'context': '. If you want to train on distributed GPUs, then you need to create multiple GPU instances with the same setting. For example, the below figure shows 8 p3dn.24xlarge instances.\\n\\n| a | search : POC\\\\_HU24 | Add filter | | | | |\\n| :unselected: | Name | | Instance ID - 4 | Instance Type | Availability Zone | Instance State v |\\n| - | - | - | - | - | - | - |\\n| :unselected: | POC\\\\_HU24\\\\_81 | | i-089bb90e91fef7b09 | p3dn.24xlarge | us-west-2c | :selected: running |\\n| :unselected: | POC\\\\_HU24\\\\_82 | | i-09be131f79506dcc1 | p3dn.24xlarge | us-west-2c | :unselected: running |\\n| :unselected: | POC\\\\_HU24\\\\_83 | | i-0c44553f8570af264 | p3dn.24xlarge | us-west-2c | :unselected: running |\\n| :unselected: | POC\\\\_HU24\\\\_84 | | i-0d8f1a29d7864e892 | p3dn.24xlarge | us-west-2c | :unselected: running |\\n| :unselected: | POC\\\\_HU24\\\\_85 | | i-0de84adf899462171 | p3dn.24xlarge | us-west-2c | :unselected: running |\\n| :unselected: | POC\\\\_HU24\\\\_86 | | i-0e56678cc29ad0de8 | p3dn.24xlarge | us-west-2c | :unselected: running |\\n| :unselected: | POC\\\\_HU24\\\\_87 | | i-Of4912fb1d7760a1b | p3dn.24xlarge | us-west-2c | :unselected: running |\\n| :unselected: | POC\\\\_HU24\\\\_88 | | i-Of4ddd1c5bbfd4dd7 | p3dn.24xlarge | us-west-2c | :selected: running |\\n\\n· Please refer to the website for the remaining steps; https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-horovod- tensorflow.html. Note that all code and all feature sets(TFRecord and RecordIO) must be on the same path on each server.\\n\\n<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\\n\\n<!-- PageNumber=\"7/9\" -->\\n\\n<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\\n\\n<!-- PageHeader=\"24\\\\. 7. 22. 오전 9:52\" -->\\n\\n\\n# · After training, please check the training log and evaluation log by checking imagenet\\\\_resnet folder:\\n\\n<figure>',\n",
       "  'domain': 'Distributed training on Cloud',\n",
       "  'num_questions': '3'},\n",
       " {'context': '<!-- PageHeader=\"24\\\\. 7. 22. 오전 9:52\" -->\\n\\n\\n# · After training, please check the training log and evaluation log by checking imagenet\\\\_resnet folder:\\n\\n<figure>\\n\\n![](figures/0)<!-- FigureContent=\"The image shows a code snippet for setting up and downloading the ImageNet dataset by exporting a username and access key. Below are the steps mentioned in the code block:\\n\\n1. Export the ImageNet username:\\n   ```\\n   export IMAGENET_USERNAME=[YOUR_USERNAME]\\n   ```\\n\\n2. Export the ImageNet access key:\\n   ```\\n   export IMAGENET_ACCESS_KEY=[YOUR_ACCESS_KEY]\\n   ```\\n\\n3. Change directory to `imagenet/data`:\\n   ```\\n   cd imagenet/data\\n   ```\\n\\n4. Move the `imagenet_2012_validation_synset_labels.txt` to `synsets.txt`:\\n   ```\\n   mv imagenet_2012_validation_synset_labels.txt synsets.txt\\n   ```\\n\\n5. Run the `download_imagenet.sh` script in the background, and redirect its output to `download.log`:\\n   ```\\n   nohup bash download_imagenet.sh . synsets.txt >& download.log &\\n   ```\\n\\nAt the end of the image, there is a heading \"Method 2 (Alternative method if Method 1 does not work).\" which indicates an alternative method for downloading the ImageNet dataset manually.\" --></figure>\\n\\n\\n<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->',\n",
       "  'domain': 'Distributed training on Cloud',\n",
       "  'num_questions': '3'},\n",
       " {'context': '<!-- PageHeader=\"[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\" -->\\n\\n| :unselected: | | | || ubuntu@ip-172-31-3-51: \\\\~ /examples/horovod/tensorflow/imagenet\\\\_resnet (ssh)  |\\n| - | - | - | - | - | - |\\n| ubuntu@ip-172-31-3-51 :\\\\~ /examples/horovod/tensorflow/imagenet\\\\_resnet$ ls -l ||||||\\n| total 646280 | | ||||\\n| 6-rw-rw-r -- 1 ubuntu ubuntu ||||| 89 Sep 23 02:54 checkpoint     |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 18880 Oct 1 01:22 eval\\\\_hvd\\\\_train. log     |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 21227745 Sep 23 03:01 events.out. tfevents. 1569199858. ip-172-31-3-51     |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 9287777 Sep 23 00:51 graph.pbtxt     |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 18880 Sep 23 03:03 hvd\\\\_train. log     |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 8 Sep 23 00:51 model.ckpt-0.data-00000-of-00002     |\\n|||||| -rw-rw-r -- 1 ubuntu ubuntu 204668736 Sep 23 00:51 model.ckpt-0.data-00001-of-00002      |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 17114 Sep 23 00:51 model. ckpt-0. index     |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 5709416 Sep 23 00:51 model. ckpt-0.meta     |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 8 Sep 23 01:53 model.ckpt-10000.data-00000-of-00002     |\\n|||||| -rw-rw-r -- 1 ubuntu ubuntu 204668736 Sep 23 01:53 model.ckpt-10000.data-00001-of-00002      |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 17114 Sep 23 01:53 model.ckpt-10000. index     |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 5709416 Sep 23 01:53 model.ckpt-10000.meta     |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 8 Sep 23 02:54 model.ckpt-20000.data-00000-of-00002     |\\n|||||| -rw-rw-r -- 1 ubuntu ubuntu 204668736 Sep 23 02:54 model.ckpt-20000.data-00001-of-00002      |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 17114 Sep 23 02:54 model.ckpt-20000. index     |\\n| -rw-rw-r -- 1 ubuntu ubuntu ||||| 5709416 Sep 23 02:54 model.ckpt-20000.meta     |\\n| (ubuntu@ip-172-31-3-51 :\\\\~ /examples/horovod/tensorflow/imagenet\\\\_resnet$ ||||||\\n\\n\\n## · vd\\\\_train\\\\_log (32 GPUS; 4 p3dn.24xlarge instances)',\n",
       "  'domain': 'Distributed training on Cloud',\n",
       "  'num_questions': '3'},\n",
       " {'context': '## · vd\\\\_train\\\\_log (32 GPUS; 4 p3dn.24xlarge instances)\\n\\n\\\\- Step Epoch Speed Loss FinLoss LR - 0 0.0 952.2 6.923 8.262\\n\\n0.00100 - 1 0.0 2686.6 6.928 8.267 0.00305 - 50 0.3 22243.7\\n\\n6.586 7.919 0.10353 14000 89.5 21021.1 0.750 1.152 -\\n\\n0.00012 - 14050 89.8 21818.7 0.583 0.985 0.00002 - Finished in 5289. 161954164505\\n\\n\\n## · eval\\\\_hvd\\\\_train.log (32 GPUS; 4 p3dn.24xlarge instances)\\n\\nubuntu@ip-172-31-3-51 :\\\\~ /examples/horovod/tensorflow$ cat eval\\\\_hvd\\\\_train\\\\_gpu32.log\\n\\nPY3.6.5 |Anaconda, Inc. | (default, Apr 29 2018, 16:14:56)\\n\\n[GCC 7.2.0]TF1.13.1\\n\\nHorovod size: 8\\n\\nUsing data from: /home/ubuntu/data1/tf-imagenet/ Evaluating Validation dataset size: 50000\\n\\nstep epoch top1 top5 loss checkpoint\\\\_time(UTC) 14075 90.2 75.821 92.90\\n\\n0.92 2019-09-20 07:50:57\\n\\nFinished evaluation\\n\\n<!-- PageFooter=\"https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\" -->\\n\\n<!-- PageNumber=\"8/9\" -->',\n",
       "  'domain': 'Distributed training on Cloud',\n",
       "  'num_questions': '3'},\n",
       " {'context': \"[Hands-on] Fast Training\\nImageNet on on-demand EC2\\nGPU instances with Horovod\\n💻\\nAuthor: Daekeun Kim (daekeun@amazon.com)\\nGoal\\nThis document is for people who need distributed GPU training using Horovod for\\nexperimental purposes. Many steps are similar to what mentioned in Julien\\nSimon’s article(\\n) and AWS\\nDocumentation(\\n). So I recommend you to view these articles first. If there\\nare some things that aren’t going well (e.g., Downloading the dataset does not\\nwork, How to convert the raw data to the TFRecord feature set?, How to fix the\\nerror ModuleNotFoundError: No module named 'cv2'? ) please refer this\\ndocument.\\nhttps://medium.com/@julsimon/imagenet-part-1-going-on-an-\\nadventure-c0a62976dc72\\nhttps://docs.aws.amazon.com/dlami/latest/devguide/tutorial-\\nhorovod-tensorflow.html\\nIntroduction\\nFor data preparation and data transformation, we do not need to use a GPU\\ninstance such as p2 and p3. Instead, we can start much cheaper instances like \\nt2.large  instance with 1.0TB EBS volume.\\nFor distributed training, we need to use multiple GPU instances like p2, p3, g3 and\\ng4.\\nYou can skip step 1 if you do not want to invent the wheel again because I have\\nstored everything in my s3 bucket.\",\n",
       "  'domain': 'Distributed training on Cloud',\n",
       "  'num_questions': '3'},\n",
       " {'context': 'g4.\\nYou can skip step 1 if you do not want to invent the wheel again because I have\\nstored everything in my s3 bucket.\\n24. 7. 22. 오전 9:52\\n[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\\nhttps://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\\n1/9',\n",
       "  'domain': 'Distributed training on Cloud',\n",
       "  'num_questions': '3'},\n",
       " {'context': '•\\ns3://dataset-image/imagenet/raw  (raw jpeg)\\n•\\ns3://dataset-image/imagenet/tfrecord  (TFRecord before resizing)\\n•\\ns3://dataset-image/imagenet/tfrecord-resized  (TFRecord after resizing to\\n224x224)\\n•\\ns3://dataset-image/imagenet/recordio  (RecordIO after resizing to\\n256x256)\\n◦The reason I did not resize 224x224 is that the below article shows\\ndifferent validation accuracy for resizing strategy.\\n◦https://forums.fast.ai/t/impact-of-image-resizing-on-model-training-time-\\nand-performance/1980\\nPlease let me know if you want to access the bucket because I did not grant any\\npublic access.\\nStep 1. Downloading and Transformation\\nSetting up an EC2 instance for Data Transformation\\n• Create an EC2 instance for storing ImageNet dataset (Ubuntu 18.04 or 16.04.\\nLinux is also available). t2.micro  is also available, but t2.large  is\\nrecommended due to memory size. Note that we do not need large storage\\nsize since we will make another EBS volume to attach the EC2 instance.\\n• Create an EBS volume (1.0TB) for ImageNet dataset and then attach the\\nvolume it to your EC2 instance. ImageNet consists of 138GB for training set\\nand 6.3GB for validation set, but we need an additional space since we need to',\n",
       "  'domain': 'Distributed training on Cloud',\n",
       "  'num_questions': '3'},\n",
       " {'context': \"volume it to your EC2 instance. ImageNet consists of 138GB for training set\\nand 6.3GB for validation set, but we need an additional space since we need to\\nextract tar files as well as need to transform it to the feature sets like TFRecord\\nand RecordIO. Here is an example command using AWS CLI.\\n$ aws ec2 create-volume \\\\ --size 1000 \\\\ --region\\n[YOUR_AWS_REGION] \\\\ --availability-zone [YOUR_AZ> \\\\ --volume-\\ntype sc1 \\\\ --tag-specifications 'ResourceType=volume,Tags=\\n[{Key=Name,Value=ImageNet}]' $ aws ec2 attach-volume \\\\ --volume-\\nid vol-[YOUR_EC2_volume_id] \\\\ --instance-id i-\\n[YOUR_EC2_instance_id] \\\\ --device /dev/sdf\\n24. 7. 22. 오전 9:52\\n[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\\nhttps://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\\n2/9\",\n",
       "  'domain': 'Distributed training on Cloud',\n",
       "  'num_questions': '3'},\n",
       " {'context': \"• You can use the TensorFlow's download\\nscript(\\n) by exporting your username and access\\nkey.\\nhttps://github.com/tensorflow/models/blob/master/research/inception/in\\nception/data/download_imagenet.sh\\n$ export IMAGENET_USERNAME=[YOUR_USERNAME] $ export\\nIMAGENET_ACCESS_KEY=[YOUR_ACCESS_KEY] $ cd imagenet/data $ mv\\nimagenet_2012_validation_synset_labels.txt synsets.txt $ nohup\\nbash download_imagenet.sh . synsets.txt >& download.log &\\nMethod 2 (Alternative method if Method 1 does not work)\\n• Download ImageNet dataset manually.\\n$ nohup wget http://www.image-\\nnet.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_train.tar & $\\nnohup wget http://www.image-\\nnet.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_bbox_train_v2.tar.\\n& $ nohup wget http://www.image-\\nnet.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_img_val.tar & $\\nnohup wget http://www.image-\\nnet.org/challenges/LSVRC/2012/nnoupb/ILSVRC2012_bbox_val_v3.tgz &\\n• Extract the validation set\\n$ mkdir validation $ mv ILSVRC2012_img_val.tar validation $ cd\\nvalidation $ tar xf ILSVRC2012_img_val.tar\\n◦After extracting the validation set, move jpeg\\nfiles(ILSVRC2012_val_00000001.JPEG, ...,\",\n",
       "  'domain': 'Distributed training on Cloud',\n",
       "  'num_questions': '3'},\n",
       " {'context': '$ mkdir validation $ mv ILSVRC2012_img_val.tar validation $ cd\\nvalidation $ tar xf ILSVRC2012_img_val.tar\\n◦After extracting the validation set, move jpeg\\nfiles(ILSVRC2012_val_00000001.JPEG, ...,\\nILSVRC2012_val_00050000.JPEG) in 1,000 directories using the following\\nscript; \\n (Each directory means the unique category like ).\\nhttps://github.com/juliensimon/aws/blob/master/mxnet/imagenet/build_vali\\ndation_tree.sh\\nn01728572\\n24. 7. 22. 오전 9:52\\n[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\\nhttps://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\\n4/9',\n",
       "  'domain': 'Distributed training on Cloud',\n",
       "  'num_questions': '3'},\n",
       " {'context': '• Please refer to \\n (code) and \\n (document).\\nhttps://github.com/aws-samples/deep-learning-\\nmodels/blob/master/utils/tensorflow/preprocess_imagenet.py\\nhttps://docs.aws.amazon.com/ko_kr/dlami/latest/devguide/tutorial-horovod-\\ntensorflow.html\\n◦\\npython preprocess_imagenet.py \\\\ --local_scratch_dir=[YOUR \\nDIRECTORY] \\\\ --imagenet_username=[imagenet account] \\\\ --\\nimagenet_access_key=[imagenet access key]\\n◦\\npython tensorflow_image_resizer.py \\\\ -d imagenet \\\\ -i [PATH TO \\nTFRECORD TRAINING DATASET] \\\\ -o [PATH TO RESIZED TFRECORD TRAINING \\nDATASET] \\\\ --subset_name train \\\\ --num_preprocess_threads 60 \\\\ --\\nnum_intra_threads 2 \\\\ --num_inter_threads 2\\n• [Additional Notes] The original document uses the small number of intra-\\nop(multiple threads within one op; for example, while doing matrix\\nmultiplication operation we can divide the op by multiple threads) and inter-\\nop(thread-pool size per one executor) such that --num_intra_threads 2 \\\\ -\\n-num_inter_threads 2 . But, you can give higher number of intra-op and inter-\\nop.\\nBacking up and Copying to S3\\n• After data transformation, create a new bucket and sync or copy feature sets\\nto the bucket.\\n• Create a snapshot of the EBS volume.',\n",
       "  'domain': 'Distributed training on Cloud',\n",
       "  'num_questions': '3'},\n",
       " {'context': 'op.\\nBacking up and Copying to S3\\n• After data transformation, create a new bucket and sync or copy feature sets\\nto the bucket.\\n• Create a snapshot of the EBS volume.\\nStep 2. Training ResNet-50 Model with\\nHorovod\\n[Before get started] If you just want to train on a single machine, you may refer to \\n (RecordIO) and \\n (TFRecord)\\nhttps://medium.com/@julsimon/imagenet-part-2-the-road-goes-ever-on-and-on-\\n578f09a749f9\\nhttps://github.com/tensorflow/models/tree/master/official/r1/resnet\\n24. 7. 22. 오전 9:52\\n[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\\nhttps://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\\n6/9',\n",
       "  'domain': 'Distributed training on Cloud',\n",
       "  'num_questions': '3'},\n",
       " {'context': '• hvd_train_log (64 GPUS; 8 p3dn.24xlarge instances)\\n- Step Epoch Speed Loss FinLoss LR - 0 0.0 1907.3 6.920 8.259\\n0.00100 - 1 0.0 5164.9 6.935 8.274 0.00920 - 50 0.6 43926.5\\n6.206 7.522 0.41119 - ... - 6950 88.9 43552.2 0.783 1.185\\n0.00125 - 7000 89.5 41958.4 0.624 1.027 0.00023 - Finished in\\n2685.1825189590454\\n• eval_hvd_train.log (64 GPUS; 8 p3dn.24xlarge instances)\\n24. 7. 22. 오전 9:52\\n[Hands-on] Fast Training ImageNet on on-demand EC2 GPU instances with Horovod\\nhttps://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca\\n9/9',\n",
       "  'domain': 'Distributed training on Cloud',\n",
       "  'num_questions': '3'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 520 ms, sys: 28.8 ms, total: 549 ms\n",
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qa_pair = chain.batch(input_batch, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'QUESTION': 'How do you format and mount an EBS volume on /data and change its owner to ec2-user?',\n",
       "  'ANSWER': 'To format and mount an EBS volume on /data and change its owner to ec2-user, you can refer to the AWS documentation at https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-using-volumes.html.'},\n",
       " {'QUESTION': 'What is the first step to download ImageNet according to the provided method?',\n",
       "  'ANSWER': 'The first step to download ImageNet according to the provided method is to go to http://www.image-net.org/, sign up, and get your own username and access key.'},\n",
       " {'QUESTION': \"What is the size of the 'ILSVRC2012_img_train.tar' file?\",\n",
       "  'ANSWER': \"The size of the 'ILSVRC2012_img_train.tar' file is 147897477120 bytes.\"},\n",
       " [{'QUESTION': \"What are the recommended EC2 instance types for distributed GPU training using Uber's Horovod or Tensorflow's DistributedStrategy?\",\n",
       "   'ANSWER': \"The recommended EC2 instance types for distributed GPU training using Uber's Horovod or Tensorflow's DistributedStrategy are p3.16xlarge or p3dn.24xlarge.\"},\n",
       "  {'QUESTION': 'What is the default root volume size for the Deep Learning AMI, and what is the recommended size increase?',\n",
       "   'ANSWER': 'The default root volume size for the Deep Learning AMI is 75GB, and it is recommended to increase it to 100GB to accommodate training logs and model checkpoints.'},\n",
       "  {'QUESTION': 'What can you do if you do not want to increase the root volume size of the EC2 instance?',\n",
       "   'ANSWER': 'If you do not want to increase the root volume size, you can delete some conda environments such as Theano, Chainer, Caffe, and Caffe2 after logging in to the EC2 instance.'}],\n",
       " [{'QUESTION': 'What instance type is used for distributed GPU training in the provided context?',\n",
       "   'ANSWER': 'The instance type used for distributed GPU training in the provided context is p3dn.24xlarge.'},\n",
       "  {'QUESTION': 'How many instances are shown in the example for distributed GPU training?',\n",
       "   'ANSWER': 'The example shows 8 instances for distributed GPU training.'},\n",
       "  {'QUESTION': 'Where can you find the remaining steps for setting up distributed training with Horovod and TensorFlow?',\n",
       "   'ANSWER': 'The remaining steps for setting up distributed training with Horovod and TensorFlow can be found at https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-horovod-tensorflow.html.'}],\n",
       " [{'QUESTION': 'What is the first step to set up and download the ImageNet dataset according to the provided code snippet?',\n",
       "   'ANSWER': 'The first step to set up and download the ImageNet dataset is to export the ImageNet username using the command `export IMAGENET_USERNAME=[YOUR_USERNAME]`.'},\n",
       "  {'QUESTION': 'Where should you navigate to after exporting the ImageNet username and access key?',\n",
       "   'ANSWER': 'After exporting the ImageNet username and access key, you should navigate to the `imagenet/data` directory using the command `cd imagenet/data`.'},\n",
       "  {'QUESTION': 'What should you do if Method 1 for downloading the ImageNet dataset does not work?',\n",
       "   'ANSWER': \"If Method 1 for downloading the ImageNet dataset does not work, you should refer to the alternative method mentioned under the heading 'Method 2 (Alternative method if Method 1 does not work).'\"}],\n",
       " {'QUESTION': \"What is the size of the 'model.ckpt-20000.data-00001-of-00002' file?\",\n",
       "  'ANSWER': \"The size of the 'model.ckpt-20000.data-00001-of-00002' file is 204668736 bytes.\"},\n",
       " {'QUESTION': 'What was the learning rate (LR) at step 50 during the vd_train_log?',\n",
       "  'ANSWER': 'The learning rate (LR) at step 50 during the vd_train_log was 0.10353.'},\n",
       " [{'QUESTION': 'What type of instances are recommended for data preparation and data transformation?',\n",
       "   'ANSWER': 'For data preparation and data transformation, it is recommended to use cheaper instances like t2.large with 1.0TB EBS volume.'},\n",
       "  {'QUESTION': 'Which instances are suggested for distributed training using Horovod?',\n",
       "   'ANSWER': 'For distributed training using Horovod, it is suggested to use multiple GPU instances like p2, p3, g3, and g4.'},\n",
       "  {'QUESTION': 'Where can you find additional information if you encounter issues such as downloading the dataset or converting raw data to the TFRecord feature set?',\n",
       "   'ANSWER': 'If you encounter issues such as downloading the dataset or converting raw data to the TFRecord feature set, you can refer to Julien Simon’s article and AWS Documentation.'}],\n",
       " {'QUESTION': 'What is the purpose of the s3 bucket mentioned in the context?',\n",
       "  'ANSWER': 'The s3 bucket is used to store everything needed for the process, allowing you to skip step 1 if you do not want to invent the wheel again.'},\n",
       " [{'QUESTION': 'What is the recommended EC2 instance type for storing the ImageNet dataset and why?',\n",
       "   'ANSWER': 'The recommended EC2 instance type for storing the ImageNet dataset is t2.large due to its memory size.'},\n",
       "  {'QUESTION': 'Why is an EBS volume of 1.0TB recommended for the ImageNet dataset?',\n",
       "   'ANSWER': 'An EBS volume of 1.0TB is recommended for the ImageNet dataset because ImageNet consists of 138GB for the training set and 6.3GB for the validation set, and additional space is needed for data processing.'},\n",
       "  {'QUESTION': 'Why was the resizing strategy of 256x256 chosen for the RecordIO format instead of 224x224?',\n",
       "   'ANSWER': 'The resizing strategy of 256x256 was chosen for the RecordIO format instead of 224x224 because an article shows different validation accuracy for different resizing strategies.'}],\n",
       " {'QUESTION': 'What is the total size of the ImageNet training and validation sets combined?',\n",
       "  'ANSWER': 'The total size of the ImageNet training and validation sets combined is 144.3GB.'},\n",
       " {'QUESTION': \"What is the first step to use TensorFlow's download script for ImageNet?\",\n",
       "  'ANSWER': \"The first step to use TensorFlow's download script for ImageNet is to export your username and access key using the commands: $ export IMAGENET_USERNAME=[YOUR_USERNAME] and $ export IMAGENET_ACCESS_KEY=[YOUR_ACCESS_KEY].\"},\n",
       " [{'QUESTION': 'What is the purpose of the script mentioned in the context?',\n",
       "   'ANSWER': 'The purpose of the script mentioned in the context is to move JPEG files from the validation set into 1,000 directories, each representing a unique category.'},\n",
       "  {'QUESTION': 'Where can the script for organizing the validation set be found?',\n",
       "   'ANSWER': 'The script for organizing the validation set can be found at the following URL: https://github.com/juliensimon/aws/blob/master/mxnet/imagenet/build_validation_tree.sh.'},\n",
       "  {'QUESTION': 'What is the initial step to prepare the validation set as described in the context?',\n",
       "   'ANSWER': \"The initial step to prepare the validation set is to create a directory named 'validation', move the ILSVRC2012_img_val.tar file into this directory, navigate into the 'validation' directory, and then extract the tar file.\"}],\n",
       " [{'QUESTION': \"What is the purpose of the script 'preprocess_imagenet.py' and how is it executed?\",\n",
       "   'ANSWER': \"The purpose of the script 'preprocess_imagenet.py' is to preprocess the ImageNet dataset. It is executed using the command: python preprocess_imagenet.py --local_scratch_dir=[YOUR DIRECTORY] --imagenet_username=[imagenet account] --imagenet_access_key=[imagenet access key].\"},\n",
       "  {'QUESTION': 'How can you resize the ImageNet dataset using the provided scripts?',\n",
       "   'ANSWER': \"You can resize the ImageNet dataset using the script 'tensorflow_image_resizer.py' with the command: python tensorflow_image_resizer.py -d imagenet -i [PATH TO TFRECORD TRAINING DATASET] -o [PATH TO RESIZED TFRECORD TRAINING DATASET] --subset_name train --num_preprocess_threads 60 --num_intra_threads 2 --num_inter_threads 2.\"},\n",
       "  {'QUESTION': 'What steps should be taken after data transformation to ensure data backup and availability?',\n",
       "   'ANSWER': 'After data transformation, you should create a new bucket and sync or copy the feature sets to the bucket. Additionally, you should create a snapshot of the EBS volume to ensure data backup and availability.'}],\n",
       " [{'QUESTION': 'What should be done after data transformation according to the context?',\n",
       "   'ANSWER': 'After data transformation, a new bucket should be created and feature sets should be synced or copied to the bucket.'},\n",
       "  {'QUESTION': 'What is the next step after creating a new bucket and syncing feature sets to it?',\n",
       "   'ANSWER': 'The next step is to create a snapshot of the EBS volume.'},\n",
       "  {'QUESTION': 'Where can you find hands-on instructions for fast training of ImageNet on on-demand EC2 GPU instances with Horovod?',\n",
       "   'ANSWER': 'Hands-on instructions for fast training of ImageNet on on-demand EC2 GPU instances with Horovod can be found at https://daekeun.notion.site/Hands-on-Fast-Training-ImageNet-on-on-demand-EC2-GPU-instances-with-Horovod-eb49f580d1304082b497d91dec887dca.'}],\n",
       " [{'QUESTION': 'What was the final loss value at step 7000 in the hvd_train_log?',\n",
       "   'ANSWER': 'The final loss value at step 7000 in the hvd_train_log was 0.624.'},\n",
       "  {'QUESTION': 'How many GPUs were used for the training mentioned in the context?',\n",
       "   'ANSWER': '64 GPUs were used for the training mentioned in the context.'},\n",
       "  {'QUESTION': 'What was the learning rate (LR) at step 50 in the hvd_train_log?',\n",
       "   'ANSWER': 'The learning rate (LR) at step 50 in the hvd_train_log was 0.41119.'}]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using only option 2, run this\n",
    "question_answer_list = qa_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save to jsonl\n",
    "\n",
    "---\n",
    "\n",
    "If you want to augment dataset, you can try Evovle-Instruct or other data augmentation techniques.<br>\n",
    "Please refer to `../evolve-instruct` and `../glan-instruct` for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from util.common_utils import convert_to_oai_format, save_jsonl\n",
    "\n",
    "output_dir = './dataset'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "system_prompt_msg = f\"\"\"You are the SME (Subject Matter Expert) in {DOMAIN}. Please answer the questions accurately. If the question is in {LANGUAGE}, write your answer in {LANGUAGE}.\"\"\"\n",
    "\n",
    "save_filename = \"imagenet-training-summary\"\n",
    "# if using option 2\n",
    "\n",
    "oai_qa_pair = convert_to_oai_format(question_answer_list, system_prompt_msg=system_prompt_msg)\n",
    "\n",
    "#save_jsonl(qa_pair, f\"{output_dir}/{save_filename}.jsonl\")\n",
    "save_jsonl(oai_qa_pair, f\"{output_dir}/{save_filename}-oai.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf {splitted_raw_data_dir} pdf_image_tmp pdf_mixed_tmp outputs_tmp images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
