{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-tuning Open Source LLM using the Azure ML Python SDK (Custom Script)\n",
    "\n",
    "### Overview\n",
    "\n",
    "There are several cases where you might want to use custom scripts without MLflow in Azure ML.\n",
    "\n",
    "#### 1. Compatibility with existing workflows\n",
    "\n",
    "There are times when you don't want to use MLflow to maintain compatibility with existing workflows or toolchains. For example:\n",
    "\n",
    "-   Customized logging solution: You already have a separate solution in place for logging and tracing (e.g. WANDB).\n",
    "-   Requiring a specific format of data logging: If you need data logging in a specific format that is not MLflow's format.\n",
    "\n",
    "#### 2. Need more granular control\n",
    "\n",
    "You need more granular control over the training and inference process. MLflow provides a lot of convenience, but sometimes it makes it difficult to have granular control.\n",
    "\n",
    "#### 3. Simple use cases\n",
    "\n",
    "If your use case is simple enough that you don't need all of MLflow's features, you might be able to get by with basic AzureML functionality. If you're working on a toy project or a simple model training task and want to get by without complex tools, start with simple code.\n",
    "\n",
    "#### 4. Security and compliance\n",
    "\n",
    "You cannot use external tools because of specific security and compliance requirements.\n",
    "\n",
    "-   Data security: You can't use external logging services or data stores due to specific data security requirements.\n",
    "-   Regulatory compliance: When data must be stored in a specific format or location due to specific regulatory compliance requirements.\n",
    "\n",
    "This notebook shows a basic example of training a model with a custom script.\n",
    "\n",
    "[Note] Please use `Python 3.10 - SDK v2 (azureml_py310_sdkv2)` conda environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config file\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: python310-sdkv2\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "module_path = \"../../0_lab_preparation\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "\n",
    "from common import check_kernel\n",
    "check_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 07:39:58,146 - logger - INFO - ===== 0. Azure ML Training Info =====\n",
      "2024-11-01 07:39:58,148 - logger - INFO - AZURE_SUBSCRIPTION_ID=09aa60b6-29c0-4956-94ef-1d91ab7cf6a9\n",
      "2024-11-01 07:39:58,149 - logger - INFO - AZURE_RESOURCE_GROUP=slm-innovation-lab\n",
      "2024-11-01 07:39:58,150 - logger - INFO - AZURE_WORKSPACE=slm-innv-eastus\n",
      "2024-11-01 07:39:58,152 - logger - INFO - AZURE_DATA_NAME=slm-innv-dataset\n",
      "2024-11-01 07:39:58,153 - logger - INFO - DATA_DIR=./dataset\n",
      "2024-11-01 07:39:58,154 - logger - INFO - CLOUD_DIR=./cloud\n",
      "2024-11-01 07:39:58,156 - logger - INFO - HF_MODEL_NAME_OR_PATH=microsoft/Phi-3.5-mini-instruct\n",
      "2024-11-01 07:39:58,157 - logger - INFO - IS_DEBUG=True\n",
      "2024-11-01 07:39:58,158 - logger - INFO - USE_LOWPRIORITY_VM=True\n",
      "2024-11-01 07:39:58,159 - logger - INFO - azure_env_name=slm-innv-lab-train-aml-env\n",
      "2024-11-01 07:39:58,161 - logger - INFO - azure_compute_cluster_name=slm-innv-lab-finetune\n",
      "2024-11-01 07:39:58,162 - logger - INFO - azure_compute_cluster_size=Standard_NC24ads_A100_v4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "from logger import logger\n",
    "from datetime import datetime\n",
    "snapshot_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "with open('config.yml') as f:\n",
    "    d = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "AZURE_SUBSCRIPTION_ID = d['config']['AZURE_SUBSCRIPTION_ID']\n",
    "AZURE_RESOURCE_GROUP = d['config']['AZURE_RESOURCE_GROUP']\n",
    "AZURE_WORKSPACE = d['config']['AZURE_WORKSPACE']\n",
    "AZURE_DATA_NAME = d['config']['AZURE_DATA_NAME']    \n",
    "DATA_DIR = d['config']['DATA_DIR']\n",
    "CLOUD_DIR = d['config']['CLOUD_DIR']\n",
    "HF_MODEL_NAME_OR_PATH = d['config']['HF_MODEL_NAME_OR_PATH']\n",
    "IS_DEBUG = d['config']['IS_DEBUG']\n",
    "USE_LOWPRIORITY_VM = d['config']['USE_LOWPRIORITY_VM']\n",
    "\n",
    "azure_env_name = d['train']['azure_env_name']  \n",
    "azure_compute_cluster_name = d['train']['azure_compute_cluster_name']\n",
    "azure_compute_cluster_size = d['train']['azure_compute_cluster_size']\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CLOUD_DIR, exist_ok=True)\n",
    "\n",
    "logger.info(\"===== 0. Azure ML Training Info =====\")\n",
    "logger.info(f\"AZURE_SUBSCRIPTION_ID={AZURE_SUBSCRIPTION_ID}\")\n",
    "logger.info(f\"AZURE_RESOURCE_GROUP={AZURE_RESOURCE_GROUP}\")\n",
    "logger.info(f\"AZURE_WORKSPACE={AZURE_WORKSPACE}\")\n",
    "logger.info(f\"AZURE_DATA_NAME={AZURE_DATA_NAME}\")\n",
    "logger.info(f\"DATA_DIR={DATA_DIR}\")\n",
    "logger.info(f\"CLOUD_DIR={CLOUD_DIR}\")\n",
    "logger.info(f\"HF_MODEL_NAME_OR_PATH={HF_MODEL_NAME_OR_PATH}\")\n",
    "logger.info(f\"IS_DEBUG={IS_DEBUG}\")\n",
    "logger.info(f\"USE_LOWPRIORITY_VM={USE_LOWPRIORITY_VM}\")\n",
    "\n",
    "logger.info(f\"azure_env_name={azure_env_name}\")\n",
    "logger.info(f\"azure_compute_cluster_name={azure_compute_cluster_name}\")\n",
    "logger.info(f\"azure_compute_cluster_size={azure_compute_cluster_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Dataset preparation\n",
    "\n",
    "---\n",
    "\n",
    "Preparing dataset is the first step in training a model. You can use the `datasets` library to load the dataset if you want to use Hugging Face datasets.<br>\n",
    "Otherwise, you can use your own dataset from previous hands-on sessions.\n",
    "\n",
    "We have prepared a dataset, [`lab1_augmented_samples.json`](lab1_augmented_samples.json), for this hands-on session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_HF_DATASETS = False # Determine if we use Hugging Face Datasets or not\n",
    "\n",
    "import json\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "from logger import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 06:09:20,706 - logger - INFO - ===== 1. Custom Dataset preparation from Lab 1.  =====\n",
      "2024-11-01 06:09:20,707 - logger - INFO - Preparing dataset.\n",
      "2024-11-01 06:09:20,756 - logger - INFO - Save dataset to ./dataset\n"
     ]
    }
   ],
   "source": [
    "if not USE_HF_DATASETS:\n",
    "\n",
    "    # Function to load data from the provided file and convert to JSONL format for single-turn conversations\n",
    "    def load_and_convert_to_jsonl(file_path, system_prompt_msg=\"You're an AI assistant.\"):\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        result = []\n",
    "        \n",
    "        for item in data:\n",
    "            jsonl_entry = {\n",
    "                \"prompt\": system_prompt_msg,\n",
    "                \"messages\": [\n",
    "                    {\"content\": item[\"input\"], \"role\": \"user\"},\n",
    "                    {\"content\": item[\"output\"], \"role\": \"assistant\"}\n",
    "                ]\n",
    "            }\n",
    "            result.append(json.dumps(jsonl_entry))\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def save_jsonl_data(jsonl_data, file_path):\n",
    "        with open(file_path, 'w') as file:\n",
    "            for entry in jsonl_data:\n",
    "                file.write(entry + '\\n')\n",
    "                \n",
    "    # Function to split data into training and testing sets\n",
    "    def split_train_test(jsonl_data, train_size=0.8):\n",
    "        # Shuffle the data\n",
    "        random.shuffle(jsonl_data)\n",
    "        \n",
    "        # Calculate split index\n",
    "        split_index = int(len(jsonl_data) * train_size)\n",
    "        \n",
    "        # Split the data\n",
    "        train_data = jsonl_data[:split_index]\n",
    "        test_data = jsonl_data[split_index:]\n",
    "        \n",
    "        return train_data, test_data            \n",
    "\n",
    "    logger.info(f\"===== 1. Custom Dataset preparation from Lab 1.  =====\")\n",
    "    logger.info(f\"Preparing dataset.\")\n",
    "    file_path = \"lab1_augmented_samples.json\"\n",
    "    system_prompt_msg = \"You are the SME (Subject Matter Expert) in Distributed training on Cloud. Please answer the questions accurately.\"\n",
    "    jsonl_dataset = load_and_convert_to_jsonl(file_path, system_prompt_msg)\n",
    "    train_dataset, test_dataset = split_train_test(jsonl_dataset, train_size=0.8)\n",
    "    logger.info(f\"Save dataset to {DATA_DIR}\")\n",
    "    save_jsonl_data(train_dataset, f\"{DATA_DIR}/train.jsonl\")\n",
    "    save_jsonl_data(test_dataset, f\"{DATA_DIR}/eval.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_HF_DATASETS:\n",
    "    logger.info(f\"===== 1. Hugging Face Dataset preparation =====\")\n",
    "    logger.info(f\"Loading dataset. It may take several minutes to load the dataset.\")\n",
    "    # Load dataset from the hub\n",
    "    dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft[:2%]\")\n",
    "\n",
    "    print(f\"Dataset size: {len(dataset)}\")\n",
    "    if IS_DEBUG:\n",
    "        logger.info(f\"Activated Debug mode. The number of sample was resampled to 1000.\")\n",
    "        dataset = dataset.select(range(1000))\n",
    "\n",
    "    logger.info(f\"Save dataset to {DATA_DIR}\")\n",
    "    dataset = dataset.train_test_split(test_size=0.2)\n",
    "    train_dataset = dataset['train']\n",
    "    train_dataset.to_json(f\"{DATA_DIR}/train.jsonl\")\n",
    "    test_dataset = dataset['test']\n",
    "    test_dataset.to_json(f\"{DATA_DIR}/eval.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Training preparation\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1. Configure workspace details\n",
    "\n",
    "To connect to a workspace, we need identifying parameters - a subscription, a resource group, and a workspace name. We will use these details in the MLClient from azure.ai.ml to get a handle on the Azure Machine Learning workspace we need. We will use the default Azure authentication for this hands-on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 06:21:00,310 - logger - INFO - ===== 2. Training preparation =====\n",
      "2024-11-01 06:21:00,314 - logger - INFO - Calling DefaultAzureCredential.\n",
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "import time\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import load_component\n",
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Data, Environment, BuildContext\n",
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml import Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError\n",
    "\n",
    "logger.info(f\"===== 2. Training preparation =====\")\n",
    "logger.info(f\"Calling DefaultAzureCredential.\")\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = None\n",
    "try:\n",
    "    ml_client = MLClient.from_config(credential)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    ml_client = MLClient(credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2. Create AzureML environment and data\n",
    "\n",
    "Azure ML defines containers (called environment asset) in which your code will run. We can use the built-in environment or build a custom environment (Docker container, conda).\n",
    "This hands-on uses conda yaml.\n",
    "\n",
    "Training data can be used as a dataset stored in the local development environment, but can also be registered as AzureML data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./cloud/conda.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CLOUD_DIR}/train/conda.yml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.10\n",
    "  - pip=24.0\n",
    "  - pip:\n",
    "    - bitsandbytes==0.43.3\n",
    "    - transformers==4.44.2\n",
    "    - peft~=0.12\n",
    "    - accelerate~=0.33\n",
    "    - trl==0.10.1\n",
    "    - einops==0.8.0\n",
    "    - datasets==2.21.0\n",
    "    - wandb==0.17.8\n",
    "    - mlflow==2.16.0\n",
    "    - azureml-mlflow==1.57.0\n",
    "    - azureml-sdk==1.57.0\n",
    "    - torchvision==0.19.0\n",
    "    - torch==2.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Docker environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {CLOUD_DIR}/train/Dockerfile\n",
    "FROM mcr.microsoft.com/aifx/acpt/stable-ubuntu2004-cu124-py310-torch241:biweekly.202410.2\n",
    "\n",
    "USER root\n",
    "\n",
    "# support Deepspeed launcher requirement of passwordless ssh login\n",
    "RUN apt-get update && apt-get -y upgrade\n",
    "RUN pip install --upgrade pip\n",
    "RUN apt-get install -y openssh-server openssh-client\n",
    "\n",
    "# Install pip dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt --no-cache-dir\n",
    "\n",
    "RUN MAX_JOBS=4 pip install flash-attn==2.6.3 --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {CLOUD_DIR}/train/requirements.txt\n",
    "azureml-acft-accelerator==0.0.63\n",
    "azureml_acft_common_components==0.0.63\n",
    "azureml-acft-contrib-hf-nlp==0.0.63\n",
    "azureml-evaluate-mlflow==0.0.63\n",
    "azureml-metrics[text]==0.0.63\n",
    "mltable==1.6.1\n",
    "mpi4py==4.0.1\n",
    "sentencepiece==0.2.0\n",
    "transformers==4.46.1\n",
    "datasets==3.1.0\n",
    "accelerate==1.1.0\n",
    "diffusers==0.31.0\n",
    "onnxruntime==1.20.0\n",
    "rouge-score==0.1.2\n",
    "sacrebleu==2.4.3\n",
    "bitsandbytes==0.44.1\n",
    "einops==0.8.0\n",
    "aiohttp==3.10.10\n",
    "peft==0.13.2\n",
    "deepspeed==0.15.3\n",
    "trl==0.12.0\n",
    "tiktoken==0.8.0\n",
    "packaging==24.1\n",
    "timm==1.0.11\n",
    "azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_or_create_environment_asset(ml_client, env_name, conda_yml=\"cloud/conda.yml\", update=False):\n",
    "    print(\"def get_or_create_environment_asset\")\n",
    "    try:\n",
    "        latest_env_version = max([int(e.version) for e in ml_client.environments.list(name=env_name)])\n",
    "        if update:\n",
    "            raise ResourceExistsError('Found Environment asset, but will update the Environment.')\n",
    "        else:\n",
    "            env_asset = ml_client.environments.get(name=env_name, version=latest_env_version)\n",
    "            logger.info(f\"Found Environment asset: {env_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        print(f\"Exception: {e}\")        \n",
    "        env_docker_image = Environment(\n",
    "            image=\"mcr.microsoft.com/azureml/curated/acft-hf-nlp-gpu:latest\",\n",
    "            conda_file=conda_yml,\n",
    "            name=env_name,\n",
    "            description=\"Environment created for llm fine-tuning.\",\n",
    "        )\n",
    "        env_asset = ml_client.environments.create_or_update(env_docker_image)\n",
    "        logger.info(f\"Created/Updated Environment asset: {env_name}\")\n",
    "        \n",
    "    return env_asset\n",
    "\n",
    "def get_or_create_docker_environment_asset(ml_client, env_name, docker_dir, update=False):\n",
    "    \n",
    "    try:\n",
    "        latest_env_version = max([int(e.version) for e in ml_client.environments.list(name=env_name)])\n",
    "        if update:\n",
    "            raise ResourceExistsError('Found Environment asset, but will update the Environment.')\n",
    "        else:\n",
    "            env_asset = ml_client.environments.get(name=env_name, version=latest_env_version)\n",
    "            logger.info(f\"Found Environment asset: {env_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        logger.info(f\"Exception: {e}\")\n",
    "        env_docker_image = Environment(\n",
    "            build=BuildContext(path=docker_dir),\n",
    "            name=env_name,\n",
    "            description=\"Environment created from a Docker context.\",\n",
    "        )\n",
    "        env_asset = ml_client.environments.create_or_update(env_docker_image)\n",
    "        logger.info(f\"Created Environment asset: {env_name}\")\n",
    "    \n",
    "    return env_asset\n",
    "\n",
    "def get_or_create_data_asset(ml_client, data_name, data_local_dir, update=False):\n",
    "    print(\"def get_or_create_data_asset\")\n",
    "    try:\n",
    "        latest_data_version = max([int(d.version) for d in ml_client.data.list(name=data_name)])\n",
    "        if update:\n",
    "            raise ResourceExistsError('Found Data asset, but will update the Data.')            \n",
    "        else:\n",
    "            data_asset = ml_client.data.get(name=data_name, version=latest_data_version)\n",
    "            logger.info(f\"Found Data asset: {data_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        data = Data(\n",
    "            path=data_local_dir,\n",
    "            type=AssetTypes.URI_FOLDER,\n",
    "            description=f\"{data_name} for fine tuning\",\n",
    "            tags={\"FineTuningType\": \"Instruction\", \"Language\": \"En\"},\n",
    "            name=data_name\n",
    "        )\n",
    "        data_asset = ml_client.data.create_or_update(data)\n",
    "        logger.info(f\"Created/Updated Data asset: {data_name}\")\n",
    "        \n",
    "    return data_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_or_create_environment_asset\n",
      "Exception: (UserError) System.Net.Http.HttpConnectionResponseContent\n",
      "Code: UserError\n",
      "Message: System.Net.Http.HttpConnectionResponseContent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 07:42:20,411 - logger - INFO - Created/Updated Environment asset: slm-innv-lab-train-aml-env\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_or_create_data_asset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 07:42:22,208 - logger - INFO - Created/Updated Data asset: slm-innv-dataset\n"
     ]
    }
   ],
   "source": [
    "#env = get_or_create_environment_asset(ml_client, azure_env_name, conda_yml=f\"{CLOUD_DIR}/conda.yml\", update=False)\n",
    "env = get_or_create_docker_environment_asset(ml_client, azure_env_name, docker_dir=f\"{CLOUD_DIR}/train\", update=False)\n",
    "data = get_or_create_data_asset(ml_client, AZURE_DATA_NAME, data_local_dir=DATA_DIR, update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Training script\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do this or open the train.py file to see the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize src_train/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Training\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1. Create the compute cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 07:48:19,645 - logger - INFO - ===== 3. Training =====\n",
      "2024-11-01 07:48:19,770 - logger - INFO - The compute cluster already exists! Reusing it for the current run\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "logger.info(f\"===== 3. Training =====\")\n",
    "### Create the compute cluster\n",
    "try:\n",
    "    compute = ml_client.compute.get(azure_compute_cluster_name)\n",
    "    logger.info(\"The compute cluster already exists! Reusing it for the current run\")\n",
    "except Exception as ex:\n",
    "    logger.info(\n",
    "        f\"Looks like the compute cluster doesn't exist. Creating a new one with compute size {azure_compute_cluster_size}!\"\n",
    "    )\n",
    "    try:\n",
    "        logger.info(\"Attempt #1 - Trying to create a dedicated compute\")\n",
    "        tier = 'LowPriority' if USE_LOWPRIORITY_VM else 'Dedicated'\n",
    "        compute = AmlCompute(\n",
    "            name=azure_compute_cluster_name,\n",
    "            size=azure_compute_cluster_size,\n",
    "            tier=tier,\n",
    "            max_instances=1,  # For multi node training set this to an integer value more than 1\n",
    "        )\n",
    "        ml_client.compute.begin_create_or_update(compute).wait()\n",
    "    except Exception as e:\n",
    "        logger.info(\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Start training job\n",
    "\n",
    "The `command` allows user to configure the following key aspects.\n",
    "\n",
    "-   `inputs` - This is the dictionary of inputs using name value pairs to the command.\n",
    "    -   `type` - The type of input. This can be a `uri_file` or `uri_folder`. The default is `uri_folder`.\n",
    "    -   `path` - The path to the file or folder. These can be local or remote files or folders. For remote files - http/https, wasb are supported.\n",
    "        -   Azure ML `data`/`dataset` or `datastore` are of type `uri_folder`. To use `data`/`dataset` as input, you can use registered dataset in the workspace using the format '<data_name>:<version>'. For e.g Input(type='uri_folder', path='my_dataset:1')\n",
    "    -   `mode` - Mode of how the data should be delivered to the compute target. Allowed values are `ro_mount`, `rw_mount` and `download`. Default is `ro_mount`\n",
    "-   `code` - This is the path where the code to run the command is located\n",
    "-   `compute` - The compute on which the command will run. You can run it on the local machine by using `local` for the compute.\n",
    "-   `command` - This is the command that needs to be run\n",
    "    in the `command` using the `${{inputs.<input_name>}}` expression. To use files or folders as inputs, we can use the `Input` class. The `Input` class supports three parameters:\n",
    "-   `environment` - This is the environment needed for the command to run. Curated (built-in) or custom environments from the workspace can be used.\n",
    "-   `instance_count` - Number of nodes. Default is 1.\n",
    "-   `distribution` - Distribution configuration for distributed training scenarios. Azure Machine Learning supports PyTorch, TensorFlow, and MPI-based distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Started: about 2024/11/1 16:56 (JST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Started: about 2024/11/1 16:56 (JST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 07:56:07,180 - logger - INFO - Started training job. Now a dedicated Compute Cluster for training is provisioned and the environment\n",
      "required for training is automatically set up from Environment.\n",
      "\n",
      "If you have set up a new custom Environment, it will take approximately 20 minutes or more to set up the Environment before provisioning the training cluster.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: cyan_soccer_90p76nflfn\n",
      "Web View: https://ml.azure.com/runs/cyan_soccer_90p76nflfn?wsid=/subscriptions/09aa60b6-29c0-4956-94ef-1d91ab7cf6a9/resourcegroups/slm-innovation-lab/workspaces/slm-innv-eastus\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: cyan_soccer_90p76nflfn\n",
      "Web View: https://ml.azure.com/runs/cyan_soccer_90p76nflfn?wsid=/subscriptions/09aa60b6-29c0-4956-94ef-1d91ab7cf6a9/resourcegroups/slm-innovation-lab/workspaces/slm-innv-eastus\n",
      "\n",
      "Warnings:\n",
      "AzureMLCompute job failed\n",
      "ExecutionFailed: [REDACTED]\n",
      "\texit_codes: 1,1,1,1\n",
      "\tAppinsights Reachable: Some(true)\n",
      "\n"
     ]
    },
    {
     "ename": "JobException",
     "evalue": "Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"4 processes failed. Please see below for their details:\\nExecution failed. User process 'Rank 0' exited with status code 1. Please check log file 'user_logs/std_log_process_0.txt' for error details. Error: [rank0]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\\\", line 101, in inner_f\\n[rank0]:     return f(*args, **kwargs)\\n[rank0]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\\\", line 370, in __init__\\n[rank0]:     with PartialState().local_main_process_first():\\n[rank0]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/contextlib.py\\\", line 142, in __exit__\\n[rank0]:     next(self.gen)\\n[rank0]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/site-packages/accelerate/state.py\\\", line 516, in local_main_process_first\\n[rank0]:     yield from self._goes_first(self.is_local_main_process)\\n[rank0]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/site-packages/accelerate/state.py\\\", line 385, in _goes_first\\n[rank0]:     self.wait_for_everyone()\\n[rank0]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/site-packages/accelerate/state.py\\\", line 374, in wait_for_everyone\\n[rank0]:     torch.distributed.barrier()\\n[rank0]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\\\", line 79, in wrapper\\n[rank0]:     return func(*args, **kwargs)\\n[rank0]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\\\", line 3936, in barrier\\n[rank0]:     work = default_pg.barrier(opts=opts)\\n[rank0]: torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:275, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.20.5\\n[rank0]: ncclInvalidUsage: This usually reflects invalid usage of NCCL library.\\n[rank0]: Last error:\\n[rank0]: Duplicate GPU detected : rank 0 and rank 1 both on CUDA device 100000\\n\\nExecution failed. User process 'Rank 1' exited with status code 1. Please check log file 'user_logs/std_log_process_1.txt' for error details. Error: [rank1]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\\\", line 101, in inner_f\\n[rank1]:     return f(*args, **kwargs)\\n[rank1]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\\\", line 370, in __init__\\n[rank1]:     with PartialState().local_main_process_first():\\n[rank1]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/contextlib.py\\\", line 135, in __enter__\\n[rank1]:     return next(self.gen)\\n[rank1]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/site-packages/accelerate/state\",\n        \"message_parameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\",\n    \"component_name\": \"CommonRuntime\"\n} ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJobException\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 32\u001b[0m\n\u001b[1;32m     26\u001b[0m returned_job \u001b[38;5;241m=\u001b[39m ml_client\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mcreate_or_update(job)\n\u001b[1;32m     27\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mStarted training job. Now a dedicated Compute Cluster for training is provisioned and the environment\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124mrequired for training is automatically set up from Environment.\u001b[39m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124mIf you have set up a new custom Environment, it will take approximately 20 minutes or more to set up the Environment before provisioning the training cluster.\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturned_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/tracing/decorator.py:94\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:289\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39mspan():\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m log_activity(\n\u001b[1;32m    287\u001b[0m             logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions\n\u001b[1;32m    288\u001b[0m         ):\n\u001b[0;32m--> 289\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(logger, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpackage_logger\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger\u001b[38;5;241m.\u001b[39mpackage_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_operations.py:818\u001b[0m, in \u001b[0;36mJobOperations.stream\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pipeline_child_job(job_object):\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineChildJobError(job_id\u001b[38;5;241m=\u001b[39mjob_object\u001b[38;5;241m.\u001b[39mid)\n\u001b[0;32m--> 818\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_logs_until_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runs_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datastore_operations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequests_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requests_pipeline\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_job_ops_helper.py:334\u001b[0m, in \u001b[0;36mstream_logs_until_completion\u001b[0;34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[0m\n\u001b[1;32m    332\u001b[0m         file_handle\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m JobException(\n\u001b[1;32m    335\u001b[0m             message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException : \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(json\u001b[38;5;241m.\u001b[39mdumps(error, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)),\n\u001b[1;32m    336\u001b[0m             target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mJOB,\n\u001b[1;32m    337\u001b[0m             no_personal_data_message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException raised on failed job.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    338\u001b[0m             error_category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mSYSTEM_ERROR,\n\u001b[1;32m    339\u001b[0m         )\n\u001b[1;32m    341\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    342\u001b[0m file_handle\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[0;31mJobException\u001b[0m: Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"4 processes failed. Please see below for their details:\\nExecution failed. User process 'Rank 0' exited with status code 1. Please check log file 'user_logs/std_log_process_0.txt' for error details. Error: [rank0]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\\\", line 101, in inner_f\\n[rank0]:     return f(*args, **kwargs)\\n[rank0]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\\\", line 370, in __init__\\n[rank0]:     with PartialState().local_main_process_first():\\n[rank0]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/contextlib.py\\\", line 142, in __exit__\\n[rank0]:     next(self.gen)\\n[rank0]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/site-packages/accelerate/state.py\\\", line 516, in local_main_process_first\\n[rank0]:     yield from self._goes_first(self.is_local_main_process)\\n[rank0]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/site-packages/accelerate/state.py\\\", line 385, in _goes_first\\n[rank0]:     self.wait_for_everyone()\\n[rank0]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/site-packages/accelerate/state.py\\\", line 374, in wait_for_everyone\\n[rank0]:     torch.distributed.barrier()\\n[rank0]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/site-packages/torch/distributed/c10d_logger.py\\\", line 79, in wrapper\\n[rank0]:     return func(*args, **kwargs)\\n[rank0]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py\\\", line 3936, in barrier\\n[rank0]:     work = default_pg.barrier(opts=opts)\\n[rank0]: torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:275, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.20.5\\n[rank0]: ncclInvalidUsage: This usually reflects invalid usage of NCCL library.\\n[rank0]: Last error:\\n[rank0]: Duplicate GPU detected : rank 0 and rank 1 both on CUDA device 100000\\n\\nExecution failed. User process 'Rank 1' exited with status code 1. Please check log file 'user_logs/std_log_process_1.txt' for error details. Error: [rank1]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py\\\", line 101, in inner_f\\n[rank1]:     return f(*args, **kwargs)\\n[rank1]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/site-packages/trl/trainer/sft_trainer.py\\\", line 370, in __init__\\n[rank1]:     with PartialState().local_main_process_first():\\n[rank1]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/contextlib.py\\\", line 135, in __enter__\\n[rank1]:     return next(self.gen)\\n[rank1]:   File \\\"/azureml-envs/azureml_60764d651158a07a9deffdcdf0ff8df0/lib/python3.10/site-packages/accelerate/state\",\n        \"message_parameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\",\n    \"component_name\": \"CommonRuntime\"\n} "
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.entities import ResourceConfiguration\n",
    "\n",
    "USE_BUILTIN_ENV = False\n",
    "str_command = \"\"\n",
    "\n",
    "if USE_BUILTIN_ENV:\n",
    "    str_env = \"azureml://registries/azureml/environments/acft-hf-nlp-gpu/versions/77\" # Use built-in Environment asset\n",
    "    str_command += \"pip install -r requirements.txt && \"\n",
    "else:\n",
    "    str_env = f\"{azure_env_name}@latest\" # Use Curated (built-in) Environment asset\n",
    "    \n",
    "str_command += \"python train.py --train_dir ${{inputs.train_dir}} \\\n",
    "            --epochs ${{inputs.epoch}} --train_batch_size ${{inputs.train_batch_size}} \\\n",
    "            --eval_batch_size ${{inputs.eval_batch_size}} --model_dir ${{inputs.model_dir}}\"\n",
    "\n",
    "logger.info(f\"Env: {str_env}\")\n",
    "logger.info(f\"Command: {str_command}\")\n",
    "\n",
    "job = command(\n",
    "    inputs=dict(\n",
    "        #train_dir=Input(type=\"uri_folder\", path=DATA_DIR), # Get data from local path\n",
    "        train_dir=Input(path=f\"{AZURE_DATA_NAME}@latest\"),  # Get data from Data asset\n",
    "        epoch=d['train']['epoch'],\n",
    "        train_batch_size=d['train']['train_batch_size'],\n",
    "        eval_batch_size=d['train']['eval_batch_size'],  \n",
    "        model_dir=d['train']['model_dir']\n",
    "    ),\n",
    "    code=\"./src_train\",  # local path where the code is stored\n",
    "    compute=azure_compute_cluster_name,\n",
    "    command=str_command,\n",
    "    environment=str_env,\n",
    "    distribution={\n",
    "        \"type\": \"PyTorch\",\n",
    "        # \"process_count_per_instance\": 1, # For multi-gpu training set this to an integer value more than 1\n",
    "        \"process_count_per_instance\": 4, # Standard_NC24ads_A100_v4 has 4 A100 GPUs and thus supports 1~4 processes per instance\n",
    "    },\n",
    ")\n",
    "\n",
    "returned_job = ml_client.jobs.create_or_update(job)\n",
    "logger.info(\"\"\"Started training job. Now a dedicated Compute Cluster for training is provisioned and the environment\n",
    "required for training is automatically set up from Environment.\n",
    "\n",
    "If you have set up a new custom Environment, it will take approximately 20 minutes or more to set up the Environment before provisioning the training cluster.\n",
    "\"\"\")\n",
    "ml_client.jobs.stream(returned_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 processes generated the above error, try use MLFlow and only 1 process instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-01 08:35:59,389 - logger - INFO - Started training job at 2024-11-01 08:35:59.389493. Now a dedicated Compute Cluster for training is provisioned and the environment\n",
      "required for training is automatically set up from Environment.\n",
      "\n",
      "If you have set up a new custom Environment, it will take approximately 20 minutes or more to set up the Environment before provisioning the training cluster.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: great_turtle_gfy5s5gyh1\n",
      "Web View: https://ml.azure.com/runs/great_turtle_gfy5s5gyh1?wsid=/subscriptions/09aa60b6-29c0-4956-94ef-1d91ab7cf6a9/resourcegroups/slm-innovation-lab/workspaces/slm-innv-eastus\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: great_turtle_gfy5s5gyh1\n",
      "Web View: https://ml.azure.com/runs/great_turtle_gfy5s5gyh1?wsid=/subscriptions/09aa60b6-29c0-4956-94ef-1d91ab7cf6a9/resourcegroups/slm-innovation-lab/workspaces/slm-innv-eastus\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.entities import ResourceConfiguration\n",
    "from datetime import datetime\n",
    "\n",
    "job = command(\n",
    "    inputs=dict(\n",
    "        #train_dir=Input(type=\"uri_folder\", path=DATA_DIR), # Get data from local path\n",
    "        train_dir=Input(path=f\"{AZURE_DATA_NAME}@latest\"),  # Get data from Data asset\n",
    "        epoch=d['train']['epoch'],\n",
    "        train_batch_size=d['train']['train_batch_size'],\n",
    "        eval_batch_size=d['train']['eval_batch_size'],  \n",
    "        model_dir=d['train']['model_dir']\n",
    "    ),\n",
    "    code=\"./src_train\",  # local path where the code is stored\n",
    "    compute=azure_compute_cluster_name,\n",
    "    command=\"python train_mlflow.py --train_dir ${{inputs.train_dir}} --epochs ${{inputs.epoch}} --train_batch_size ${{inputs.train_batch_size}} --eval_batch_size ${{inputs.eval_batch_size}} --model_dir ${{inputs.model_dir}}\",\n",
    "    #environment=\"azureml://registries/azureml/environments/acft-hf-nlp-gpu/versions/57\", # Use built-in Environment asset\n",
    "    environment=f\"{azure_env_name}@latest\",\n",
    "    distribution={\n",
    "        \"type\": \"PyTorch\",\n",
    "        \"process_count_per_instance\": 1, # For multi-gpu training set this to an integer value more than 1\n",
    "        # \"process_count_per_instance\": 4, # Standard_NC24ads_A100_v4 has 4 A100 GPUs and thus supports 1~4 processes per instance\n",
    "    },\n",
    ")\n",
    "\n",
    "returned_job = ml_client.jobs.create_or_update(job)\n",
    "logger.info(f\"\"\"Started training job at {datetime.now()}. Now a dedicated Compute Cluster for training is provisioned and the environment\n",
    "required for training is automatically set up from Environment.\n",
    "\n",
    "If you have set up a new custom Environment, it will take approximately 20 minutes or more to set up the Environment before provisioning the training cluster.\n",
    "\"\"\")\n",
    "ml_client.jobs.stream(returned_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(returned_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the `trained_model` output is available\n",
    "job_name = returned_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. (Optional) Create model asset and get fine-tuned LLM to local folder\n",
    "\n",
    "---\n",
    "\n",
    "### 4.1. Create model asset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_model_asset(ml_client, model_name, job_name, model_dir=\"outputs\", model_type=\"custom_model\", update=False):\n",
    "    \n",
    "    try:\n",
    "        latest_model_version = max([int(m.version) for m in ml_client.models.list(name=model_name)])\n",
    "        if update:\n",
    "            raise ResourceExistsError('Found Model asset, but will update the Model.')\n",
    "        else:\n",
    "            model_asset = ml_client.models.get(name=model_name, version=latest_model_version)\n",
    "            logger.info(f\"Found Model asset: {model_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        logger.info(f\"Exception: {e}\")        \n",
    "        model_path = f\"azureml://jobs/{job_name}/outputs/artifacts/paths/{model_dir}/\"    \n",
    "        run_model = Model(\n",
    "            name=model_name,        \n",
    "            path=model_path,\n",
    "            description=\"Model created from run.\",\n",
    "            type=model_type # mlflow_model, custom_model, triton_model\n",
    "        )\n",
    "        model_asset = ml_client.models.create_or_update(run_model)\n",
    "        logger.info(f\"Created Model asset: {model_name}\")\n",
    "\n",
    "    return model_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_model_name = d['serve']['azure_model_name']\n",
    "model_dir = d['train']['model_dir']\n",
    "model = get_or_create_model_asset(ml_client, azure_model_name, job_name, model_dir, model_type=\"custom_model\", update=False)\n",
    "\n",
    "logger.info(\"===== 4. (Optional) Create model asset and get fine-tuned LLM to local folder =====\")\n",
    "logger.info(f\"azure_model_name={azure_model_name}\")\n",
    "logger.info(f\"model_dir={model_dir}\")\n",
    "logger.info(f\"model={model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Get fine-tuned LLM to local folder\n",
    "\n",
    "You can copy it to your local directory to perform inference or serve the model in Azure environment. (e.g., real-time endpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download the model (this is optional)\n",
    "local_model_dir = \"./artifact_downloads\"\n",
    "os.makedirs(local_model_dir, exist_ok=True)\n",
    "\n",
    "ml_client.models.download(name=azure_model_name, download_path=local_model_dir, version=model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $DATA_DIR {local_model_dir}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
